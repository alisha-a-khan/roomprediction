{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b33de1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Graph 1 Loss: 6.53779, Graph 2 Loss: 22.88020\n",
      "Epoch: 001, Graph 1 Loss: 9.35793, Graph 2 Loss: 10.16735\n",
      "Epoch: 002, Graph 1 Loss: 3.59483, Graph 2 Loss: 14.60684\n",
      "Epoch: 003, Graph 1 Loss: 0.74036, Graph 2 Loss: 6.35166\n",
      "Epoch: 004, Graph 1 Loss: 2.84047, Graph 2 Loss: 7.02935\n",
      "Epoch: 005, Graph 1 Loss: 1.31023, Graph 2 Loss: 0.80142\n",
      "Epoch: 006, Graph 1 Loss: 2.33960, Graph 2 Loss: 6.88519\n",
      "Epoch: 007, Graph 1 Loss: 1.20018, Graph 2 Loss: 3.97843\n",
      "Epoch: 008, Graph 1 Loss: 0.74760, Graph 2 Loss: 4.28111\n",
      "Epoch: 009, Graph 1 Loss: 1.49614, Graph 2 Loss: 5.80094\n",
      "Epoch: 010, Graph 1 Loss: 2.36446, Graph 2 Loss: 3.01323\n",
      "Epoch: 011, Graph 1 Loss: 2.70807, Graph 2 Loss: 5.11795\n",
      "Epoch: 012, Graph 1 Loss: 1.17502, Graph 2 Loss: 1.57652\n",
      "Epoch: 013, Graph 1 Loss: 3.92836, Graph 2 Loss: 1.28823\n",
      "Epoch: 014, Graph 1 Loss: 2.29801, Graph 2 Loss: 2.79139\n",
      "Epoch: 015, Graph 1 Loss: 2.06243, Graph 2 Loss: 1.73660\n",
      "Epoch: 016, Graph 1 Loss: 6.57269, Graph 2 Loss: 2.34772\n",
      "Epoch: 017, Graph 1 Loss: 2.25845, Graph 2 Loss: 2.61376\n",
      "Epoch: 018, Graph 1 Loss: 5.35761, Graph 2 Loss: 1.55616\n",
      "Epoch: 019, Graph 1 Loss: 2.16062, Graph 2 Loss: 1.37844\n",
      "Epoch: 020, Graph 1 Loss: 2.68566, Graph 2 Loss: 2.10660\n",
      "Epoch: 021, Graph 1 Loss: 1.14194, Graph 2 Loss: 1.36007\n",
      "Epoch: 022, Graph 1 Loss: 1.34443, Graph 2 Loss: 4.55248\n",
      "Epoch: 023, Graph 1 Loss: 2.04977, Graph 2 Loss: 1.07928\n",
      "Epoch: 024, Graph 1 Loss: 5.06864, Graph 2 Loss: 4.08542\n",
      "Epoch: 025, Graph 1 Loss: 0.81005, Graph 2 Loss: 3.04884\n",
      "Epoch: 026, Graph 1 Loss: 1.54694, Graph 2 Loss: 3.24604\n",
      "Epoch: 027, Graph 1 Loss: 2.30821, Graph 2 Loss: 0.87354\n",
      "Epoch: 028, Graph 1 Loss: 1.15040, Graph 2 Loss: 1.55067\n",
      "Epoch: 029, Graph 1 Loss: 2.56119, Graph 2 Loss: 1.48472\n",
      "Epoch: 030, Graph 1 Loss: 1.12166, Graph 2 Loss: 1.68256\n",
      "Epoch: 031, Graph 1 Loss: 2.96525, Graph 2 Loss: 1.04959\n",
      "Epoch: 032, Graph 1 Loss: 1.82242, Graph 2 Loss: 1.42127\n",
      "Epoch: 033, Graph 1 Loss: 2.15982, Graph 2 Loss: 1.43002\n",
      "Epoch: 034, Graph 1 Loss: 2.89665, Graph 2 Loss: 1.70766\n",
      "Epoch: 035, Graph 1 Loss: 1.70595, Graph 2 Loss: 1.24006\n",
      "Epoch: 036, Graph 1 Loss: 1.39099, Graph 2 Loss: 1.50576\n",
      "Epoch: 037, Graph 1 Loss: 1.83775, Graph 2 Loss: 0.79308\n",
      "Epoch: 038, Graph 1 Loss: 1.11425, Graph 2 Loss: 0.96617\n",
      "Epoch: 039, Graph 1 Loss: 1.16991, Graph 2 Loss: 1.42219\n",
      "Epoch: 040, Graph 1 Loss: 1.64731, Graph 2 Loss: 1.39836\n",
      "Epoch: 041, Graph 1 Loss: 2.29464, Graph 2 Loss: 1.26707\n",
      "Epoch: 042, Graph 1 Loss: 0.97255, Graph 2 Loss: 1.46018\n",
      "Epoch: 043, Graph 1 Loss: 1.32843, Graph 2 Loss: 0.86117\n",
      "Epoch: 044, Graph 1 Loss: 1.18774, Graph 2 Loss: 1.71508\n",
      "Epoch: 045, Graph 1 Loss: 1.01857, Graph 2 Loss: 0.89108\n",
      "Epoch: 046, Graph 1 Loss: 1.02358, Graph 2 Loss: 0.90456\n",
      "Epoch: 047, Graph 1 Loss: 0.81317, Graph 2 Loss: 1.51498\n",
      "Epoch: 048, Graph 1 Loss: 0.86594, Graph 2 Loss: 1.43672\n",
      "Epoch: 049, Graph 1 Loss: 1.61048, Graph 2 Loss: 0.75895\n",
      "Epoch: 050, Graph 1 Loss: 1.53701, Graph 2 Loss: 1.76053\n",
      "Epoch: 051, Graph 1 Loss: 1.10497, Graph 2 Loss: 1.38327\n",
      "Epoch: 052, Graph 1 Loss: 1.02176, Graph 2 Loss: 1.17702\n",
      "Epoch: 053, Graph 1 Loss: 0.96881, Graph 2 Loss: 0.81388\n",
      "Epoch: 054, Graph 1 Loss: 1.07939, Graph 2 Loss: 1.62742\n",
      "Epoch: 055, Graph 1 Loss: 0.84200, Graph 2 Loss: 0.99638\n",
      "Epoch: 056, Graph 1 Loss: 0.78997, Graph 2 Loss: 1.33487\n",
      "Epoch: 057, Graph 1 Loss: 1.07025, Graph 2 Loss: 1.54741\n",
      "Epoch: 058, Graph 1 Loss: 0.86527, Graph 2 Loss: 1.32307\n",
      "Epoch: 059, Graph 1 Loss: 0.76464, Graph 2 Loss: 1.07089\n",
      "Epoch: 060, Graph 1 Loss: 0.91505, Graph 2 Loss: 0.97055\n",
      "Epoch: 061, Graph 1 Loss: 0.87213, Graph 2 Loss: 1.26352\n",
      "Epoch: 062, Graph 1 Loss: 1.05886, Graph 2 Loss: 0.93247\n",
      "Epoch: 063, Graph 1 Loss: 1.21268, Graph 2 Loss: 0.97029\n",
      "Epoch: 064, Graph 1 Loss: 1.05610, Graph 2 Loss: 2.02153\n",
      "Epoch: 065, Graph 1 Loss: 1.05412, Graph 2 Loss: 1.22761\n",
      "Epoch: 066, Graph 1 Loss: 0.95289, Graph 2 Loss: 1.17554\n",
      "Epoch: 067, Graph 1 Loss: 0.82076, Graph 2 Loss: 1.13035\n",
      "Epoch: 068, Graph 1 Loss: 0.95695, Graph 2 Loss: 0.91843\n",
      "Epoch: 069, Graph 1 Loss: 1.50223, Graph 2 Loss: 0.91163\n",
      "Epoch: 070, Graph 1 Loss: 0.90798, Graph 2 Loss: 1.02015\n",
      "Epoch: 071, Graph 1 Loss: 0.86120, Graph 2 Loss: 1.03493\n",
      "Epoch: 072, Graph 1 Loss: 1.09038, Graph 2 Loss: 0.81785\n",
      "Epoch: 073, Graph 1 Loss: 1.04822, Graph 2 Loss: 1.27248\n",
      "Epoch: 074, Graph 1 Loss: 1.01008, Graph 2 Loss: 0.82983\n",
      "Epoch: 075, Graph 1 Loss: 1.02272, Graph 2 Loss: 0.86669\n",
      "Epoch: 076, Graph 1 Loss: 1.02862, Graph 2 Loss: 1.26792\n",
      "Epoch: 077, Graph 1 Loss: 1.14086, Graph 2 Loss: 0.97931\n",
      "Epoch: 078, Graph 1 Loss: 1.04697, Graph 2 Loss: 1.35437\n",
      "Epoch: 079, Graph 1 Loss: 1.09528, Graph 2 Loss: 0.82536\n",
      "Epoch: 080, Graph 1 Loss: 1.10381, Graph 2 Loss: 0.86490\n",
      "Epoch: 081, Graph 1 Loss: 1.00075, Graph 2 Loss: 1.26760\n",
      "Epoch: 082, Graph 1 Loss: 0.85002, Graph 2 Loss: 0.82140\n",
      "Epoch: 083, Graph 1 Loss: 1.14204, Graph 2 Loss: 1.12276\n",
      "Epoch: 084, Graph 1 Loss: 0.91416, Graph 2 Loss: 0.97877\n",
      "Epoch: 085, Graph 1 Loss: 1.38577, Graph 2 Loss: 1.39770\n",
      "Epoch: 086, Graph 1 Loss: 0.99237, Graph 2 Loss: 0.96631\n",
      "Epoch: 087, Graph 1 Loss: 1.03613, Graph 2 Loss: 1.17086\n",
      "Epoch: 088, Graph 1 Loss: 1.05216, Graph 2 Loss: 1.65934\n",
      "Epoch: 089, Graph 1 Loss: 0.95280, Graph 2 Loss: 0.89959\n",
      "Epoch: 090, Graph 1 Loss: 1.32889, Graph 2 Loss: 1.36550\n",
      "Epoch: 091, Graph 1 Loss: 0.98137, Graph 2 Loss: 0.93582\n",
      "Epoch: 092, Graph 1 Loss: 1.02864, Graph 2 Loss: 1.12196\n",
      "Epoch: 093, Graph 1 Loss: 0.87485, Graph 2 Loss: 0.95538\n",
      "Epoch: 094, Graph 1 Loss: 0.91691, Graph 2 Loss: 0.89951\n",
      "Epoch: 095, Graph 1 Loss: 0.88639, Graph 2 Loss: 1.05070\n",
      "Epoch: 096, Graph 1 Loss: 1.00945, Graph 2 Loss: 1.27510\n",
      "Epoch: 097, Graph 1 Loss: 1.05265, Graph 2 Loss: 1.08266\n",
      "Epoch: 098, Graph 1 Loss: 0.98670, Graph 2 Loss: 1.00326\n",
      "Epoch: 099, Graph 1 Loss: 1.36163, Graph 2 Loss: 1.09564\n",
      "Epoch: 100, Graph 1 Loss: 1.25690, Graph 2 Loss: 0.98642\n",
      "Epoch: 101, Graph 1 Loss: 1.00708, Graph 2 Loss: 0.96347\n",
      "Epoch: 102, Graph 1 Loss: 1.09950, Graph 2 Loss: 0.93660\n",
      "Epoch: 103, Graph 1 Loss: 0.92178, Graph 2 Loss: 1.07006\n",
      "Epoch: 104, Graph 1 Loss: 1.07982, Graph 2 Loss: 1.26906\n",
      "Epoch: 105, Graph 1 Loss: 1.28513, Graph 2 Loss: 1.14658\n",
      "Epoch: 106, Graph 1 Loss: 0.81178, Graph 2 Loss: 1.09961\n",
      "Epoch: 107, Graph 1 Loss: 1.12921, Graph 2 Loss: 1.20247\n",
      "Epoch: 108, Graph 1 Loss: 1.18016, Graph 2 Loss: 1.16423\n",
      "Epoch: 109, Graph 1 Loss: 1.26753, Graph 2 Loss: 0.94782\n",
      "Epoch: 110, Graph 1 Loss: 1.13398, Graph 2 Loss: 0.91288\n",
      "Epoch: 111, Graph 1 Loss: 1.09033, Graph 2 Loss: 0.97513\n",
      "Epoch: 112, Graph 1 Loss: 1.05158, Graph 2 Loss: 0.90956\n",
      "Epoch: 113, Graph 1 Loss: 1.17756, Graph 2 Loss: 1.02011\n",
      "Epoch: 114, Graph 1 Loss: 1.11266, Graph 2 Loss: 1.08280\n",
      "Epoch: 115, Graph 1 Loss: 1.08471, Graph 2 Loss: 1.08621\n",
      "Epoch: 116, Graph 1 Loss: 0.90752, Graph 2 Loss: 1.04712\n",
      "Epoch: 117, Graph 1 Loss: 0.85354, Graph 2 Loss: 1.17946\n",
      "Epoch: 118, Graph 1 Loss: 0.95426, Graph 2 Loss: 0.85296\n",
      "Epoch: 119, Graph 1 Loss: 0.98385, Graph 2 Loss: 1.32823\n",
      "Epoch: 120, Graph 1 Loss: 1.10347, Graph 2 Loss: 0.92462\n",
      "Epoch: 121, Graph 1 Loss: 1.07214, Graph 2 Loss: 1.24622\n",
      "Epoch: 122, Graph 1 Loss: 0.91415, Graph 2 Loss: 1.34304\n",
      "Epoch: 123, Graph 1 Loss: 0.95137, Graph 2 Loss: 1.19958\n",
      "Epoch: 124, Graph 1 Loss: 1.08078, Graph 2 Loss: 1.09062\n",
      "Epoch: 125, Graph 1 Loss: 1.09042, Graph 2 Loss: 0.96723\n",
      "Epoch: 126, Graph 1 Loss: 1.21274, Graph 2 Loss: 1.06522\n",
      "Epoch: 127, Graph 1 Loss: 1.20997, Graph 2 Loss: 1.05659\n",
      "Epoch: 128, Graph 1 Loss: 1.19877, Graph 2 Loss: 0.90814\n",
      "Epoch: 129, Graph 1 Loss: 1.20951, Graph 2 Loss: 1.09275\n",
      "Epoch: 130, Graph 1 Loss: 1.18898, Graph 2 Loss: 1.02643\n",
      "Epoch: 131, Graph 1 Loss: 1.04485, Graph 2 Loss: 1.10854\n",
      "Epoch: 132, Graph 1 Loss: 1.10529, Graph 2 Loss: 1.05937\n",
      "Epoch: 133, Graph 1 Loss: 0.98687, Graph 2 Loss: 0.97202\n",
      "Epoch: 134, Graph 1 Loss: 1.02329, Graph 2 Loss: 1.03109\n",
      "Epoch: 135, Graph 1 Loss: 1.02107, Graph 2 Loss: 1.07969\n",
      "Epoch: 136, Graph 1 Loss: 1.15911, Graph 2 Loss: 1.03741\n",
      "Epoch: 137, Graph 1 Loss: 1.02609, Graph 2 Loss: 1.02460\n",
      "Epoch: 138, Graph 1 Loss: 1.07839, Graph 2 Loss: 1.05842\n",
      "Epoch: 139, Graph 1 Loss: 1.06346, Graph 2 Loss: 0.99436\n",
      "Epoch: 140, Graph 1 Loss: 1.08773, Graph 2 Loss: 1.00452\n",
      "Epoch: 141, Graph 1 Loss: 1.16978, Graph 2 Loss: 1.08073\n",
      "Epoch: 142, Graph 1 Loss: 1.09143, Graph 2 Loss: 1.02302\n",
      "Epoch: 143, Graph 1 Loss: 1.20139, Graph 2 Loss: 0.99939\n",
      "Epoch: 144, Graph 1 Loss: 1.08172, Graph 2 Loss: 1.00142\n",
      "Epoch: 145, Graph 1 Loss: 1.06170, Graph 2 Loss: 1.08670\n",
      "Epoch: 146, Graph 1 Loss: 1.13084, Graph 2 Loss: 1.01485\n",
      "Epoch: 147, Graph 1 Loss: 1.10920, Graph 2 Loss: 0.99302\n",
      "Epoch: 148, Graph 1 Loss: 1.05319, Graph 2 Loss: 1.01716\n",
      "Epoch: 149, Graph 1 Loss: 1.07500, Graph 2 Loss: 0.98768\n",
      "Epoch: 150, Graph 1 Loss: 1.09676, Graph 2 Loss: 1.06559\n",
      "Epoch: 151, Graph 1 Loss: 1.05913, Graph 2 Loss: 1.02568\n",
      "Epoch: 152, Graph 1 Loss: 1.08492, Graph 2 Loss: 0.99666\n",
      "Epoch: 153, Graph 1 Loss: 1.07103, Graph 2 Loss: 1.01449\n",
      "Epoch: 154, Graph 1 Loss: 1.04769, Graph 2 Loss: 1.08172\n",
      "Epoch: 155, Graph 1 Loss: 1.06153, Graph 2 Loss: 1.02340\n",
      "Epoch: 156, Graph 1 Loss: 0.99423, Graph 2 Loss: 1.11578\n",
      "Epoch: 157, Graph 1 Loss: 0.97867, Graph 2 Loss: 0.99358\n",
      "Epoch: 158, Graph 1 Loss: 1.04116, Graph 2 Loss: 1.00166\n",
      "Epoch: 159, Graph 1 Loss: 1.06024, Graph 2 Loss: 1.05950\n",
      "Epoch: 160, Graph 1 Loss: 1.03765, Graph 2 Loss: 1.04426\n",
      "Epoch: 161, Graph 1 Loss: 1.07994, Graph 2 Loss: 0.98344\n",
      "Epoch: 162, Graph 1 Loss: 1.13737, Graph 2 Loss: 0.98865\n",
      "Epoch: 163, Graph 1 Loss: 0.96805, Graph 2 Loss: 1.00667\n",
      "Epoch: 164, Graph 1 Loss: 1.00355, Graph 2 Loss: 0.95685\n",
      "Epoch: 165, Graph 1 Loss: 1.03741, Graph 2 Loss: 1.00079\n",
      "Epoch: 166, Graph 1 Loss: 1.08220, Graph 2 Loss: 1.00030\n",
      "Epoch: 167, Graph 1 Loss: 1.19335, Graph 2 Loss: 0.88587\n",
      "Epoch: 168, Graph 1 Loss: 1.07928, Graph 2 Loss: 0.99869\n",
      "Epoch: 169, Graph 1 Loss: 1.00650, Graph 2 Loss: 1.00839\n",
      "Epoch: 170, Graph 1 Loss: 1.02954, Graph 2 Loss: 0.89197\n",
      "Epoch: 171, Graph 1 Loss: 1.06498, Graph 2 Loss: 0.96857\n",
      "Epoch: 172, Graph 1 Loss: 1.19539, Graph 2 Loss: 1.04862\n",
      "Epoch: 173, Graph 1 Loss: 0.99252, Graph 2 Loss: 0.93085\n",
      "Epoch: 174, Graph 1 Loss: 1.10743, Graph 2 Loss: 1.04731\n",
      "Epoch: 175, Graph 1 Loss: 0.91299, Graph 2 Loss: 1.10979\n",
      "Epoch: 176, Graph 1 Loss: 0.99765, Graph 2 Loss: 1.05052\n",
      "Epoch: 177, Graph 1 Loss: 0.90204, Graph 2 Loss: 0.96215\n",
      "Epoch: 178, Graph 1 Loss: 1.05389, Graph 2 Loss: 1.00397\n",
      "Epoch: 179, Graph 1 Loss: 1.06471, Graph 2 Loss: 1.29479\n",
      "Epoch: 180, Graph 1 Loss: 1.06653, Graph 2 Loss: 1.03625\n",
      "Epoch: 181, Graph 1 Loss: 0.95097, Graph 2 Loss: 1.00788\n",
      "Epoch: 182, Graph 1 Loss: 0.97255, Graph 2 Loss: 1.13641\n",
      "Epoch: 183, Graph 1 Loss: 1.07100, Graph 2 Loss: 1.05405\n",
      "Epoch: 184, Graph 1 Loss: 0.86327, Graph 2 Loss: 1.06428\n",
      "Epoch: 185, Graph 1 Loss: 1.18812, Graph 2 Loss: 1.02708\n",
      "Epoch: 186, Graph 1 Loss: 1.25397, Graph 2 Loss: 1.17531\n",
      "Epoch: 187, Graph 1 Loss: 0.98158, Graph 2 Loss: 1.04100\n",
      "Epoch: 188, Graph 1 Loss: 1.07847, Graph 2 Loss: 1.02008\n",
      "Epoch: 189, Graph 1 Loss: 0.97163, Graph 2 Loss: 1.08152\n",
      "Epoch: 190, Graph 1 Loss: 1.05989, Graph 2 Loss: 1.08368\n",
      "Epoch: 191, Graph 1 Loss: 1.02120, Graph 2 Loss: 1.00335\n",
      "Epoch: 192, Graph 1 Loss: 0.91703, Graph 2 Loss: 0.95401\n",
      "Epoch: 193, Graph 1 Loss: 0.94637, Graph 2 Loss: 0.99539\n",
      "Epoch: 194, Graph 1 Loss: 0.93636, Graph 2 Loss: 1.01204\n",
      "Epoch: 195, Graph 1 Loss: 0.93265, Graph 2 Loss: 1.01153\n",
      "Epoch: 196, Graph 1 Loss: 0.96630, Graph 2 Loss: 0.96546\n",
      "Epoch: 197, Graph 1 Loss: 1.08876, Graph 2 Loss: 1.02159\n",
      "Epoch: 198, Graph 1 Loss: 1.05308, Graph 2 Loss: 1.04000\n",
      "Epoch: 199, Graph 1 Loss: 0.92268, Graph 2 Loss: 0.93179\n",
      "Test Loss: 1.37216, Test Accuracy: 0.00000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Define the edge indices and node features for graph 1\n",
    "g1_edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long)\n",
    "g1_x = torch.tensor([[12], [23]], dtype=torch.float)\n",
    "g1_y = ['kitchen', 'bathroom']\n",
    "\n",
    "# Convert the NetworkX graph to a PyTorch geometric data object\n",
    "g1_data = Data(x=g1_x, edge_index=g1_edge_index.t().contiguous(), y=None)\n",
    "\n",
    "# Define the edge indices and node features for graph 2\n",
    "g2_edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long)\n",
    "g2_x = torch.tensor([[12], [23]], dtype=torch.float)\n",
    "g2_y = ['bedroom', 'bathroom']\n",
    "\n",
    "# Convert the NetworkX graph to a PyTorch geometric data object\n",
    "g2_data = Data(x=g2_x, edge_index=g2_edge_index.t().contiguous(), y=None)\n",
    "\n",
    "# Define the test data\n",
    "test_edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long)\n",
    "test_x = torch.tensor([[12], [23]], dtype=torch.float)\n",
    "test_y = ['bedroom', 'kitchen']\n",
    "\n",
    "# Convert the NetworkX graph to a PyTorch geometric data object\n",
    "test_data = Data(x=test_x, edge_index=test_edge_index.t().contiguous(), y=None)\n",
    "\n",
    "# Define a dictionary to map room names to integer labels\n",
    "label_dict = {'kitchen': 0, 'bathroom': 1, 'bedroom': 2}\n",
    "\n",
    "# Convert the room names to integer labels\n",
    "g1_data.y = torch.tensor([label_dict[y] for y in g1_y], dtype=torch.long)\n",
    "g2_data.y = torch.tensor([label_dict[y] for y in g2_y], dtype=torch.long)\n",
    "test_data.y = torch.tensor([label_dict[y] for y in test_y], dtype=torch.long)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(1, 16)\n",
    "        self.conv2 = GCNConv(16, 3)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss = F.nll_loss(model(data.x.to(device), data.edge_index.to(device)), data.y.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss1 = train(g1_data)\n",
    "    loss2 = train(g2_data)\n",
    "    print('Epoch: {:03d}, Graph 1 Loss: {:.5f}, Graph 2 Loss: {:.5f}'.format(epoch, loss1, loss2))\n",
    "    \n",
    "    \n",
    "def test(data):\n",
    "    model.eval()\n",
    "    logits = model(data.x.to(device), data.edge_index.to(device))\n",
    "    pred = logits.argmax(dim=1)\n",
    "    test_loss = F.nll_loss(logits, data.y.to(device))\n",
    "    test_acc = pred.eq(data.y.to(device)).sum().item() / len(data.y)\n",
    "    return test_loss.item(), test_acc\n",
    "\n",
    "test_loss, test_acc = test(test_data)\n",
    "print('Test Loss: {:.5f}, Test Accuracy: {:.5f}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8eab50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/ignore_outputs/data.pkl', 'rb') as f:\n",
    "    embeddings, Y, df, X_all, edges_df_list, attributes_df_list = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55a76cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/ignore_outputs/test_data.pkl', 'rb') as f:\n",
    "    test_embeddings, test_Y, test_df, test_X_all, test_edges_df_list, test_attributes_df_list = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7047b57e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alishakhan/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 23515.63256\n",
      "Epoch: 001, Loss: 21054.25994\n",
      "Epoch: 002, Loss: 18218.35352\n",
      "Epoch: 003, Loss: 11709.67205\n",
      "Epoch: 004, Loss: 10716.26456\n",
      "Epoch: 005, Loss: 9867.95055\n",
      "Epoch: 006, Loss: 6936.04987\n",
      "Epoch: 007, Loss: 6568.32253\n",
      "Epoch: 008, Loss: 7471.30269\n",
      "Epoch: 009, Loss: 5654.76778\n",
      "Epoch: 010, Loss: 3556.99171\n",
      "Epoch: 011, Loss: 3224.09651\n",
      "Epoch: 012, Loss: 3891.99986\n",
      "Epoch: 013, Loss: 3207.75101\n",
      "Epoch: 014, Loss: 3070.81452\n",
      "Epoch: 015, Loss: 2954.74326\n",
      "Epoch: 016, Loss: 2626.25652\n",
      "Epoch: 017, Loss: 2581.87993\n",
      "Epoch: 018, Loss: 1944.55977\n",
      "Epoch: 019, Loss: 1563.09123\n",
      "Epoch: 020, Loss: 1880.79916\n",
      "Epoch: 021, Loss: 1535.99977\n",
      "Epoch: 022, Loss: 1441.22153\n",
      "Epoch: 023, Loss: 1306.62601\n",
      "Epoch: 024, Loss: 1042.31021\n",
      "Epoch: 025, Loss: 928.41926\n",
      "Epoch: 026, Loss: 706.12984\n",
      "Epoch: 027, Loss: 753.73809\n",
      "Epoch: 028, Loss: 868.83910\n",
      "Epoch: 029, Loss: 545.39820\n",
      "Epoch: 030, Loss: 541.76747\n",
      "Epoch: 031, Loss: 503.56068\n",
      "Epoch: 032, Loss: 549.88308\n",
      "Epoch: 033, Loss: 338.69576\n",
      "Epoch: 034, Loss: 389.59650\n",
      "Epoch: 035, Loss: 417.90479\n",
      "Epoch: 036, Loss: 322.90878\n",
      "Epoch: 037, Loss: 645.99485\n",
      "Epoch: 038, Loss: 358.96244\n",
      "Epoch: 039, Loss: 322.65869\n",
      "Epoch: 040, Loss: 362.00813\n",
      "Epoch: 041, Loss: 278.38421\n",
      "Epoch: 042, Loss: 164.00551\n",
      "Epoch: 043, Loss: 161.35954\n",
      "Epoch: 044, Loss: 258.45591\n",
      "Epoch: 045, Loss: 231.86193\n",
      "Epoch: 046, Loss: 200.01871\n",
      "Epoch: 047, Loss: 154.26453\n",
      "Epoch: 048, Loss: 150.19883\n",
      "Epoch: 049, Loss: 153.96391\n",
      "Epoch: 050, Loss: 103.96000\n",
      "Epoch: 051, Loss: 115.82195\n",
      "Epoch: 052, Loss: 92.72995\n",
      "Epoch: 053, Loss: 89.58861\n",
      "Epoch: 054, Loss: 129.06591\n",
      "Epoch: 055, Loss: 101.76640\n",
      "Epoch: 056, Loss: 76.49112\n",
      "Epoch: 057, Loss: 90.52256\n",
      "Epoch: 058, Loss: 90.15237\n",
      "Epoch: 059, Loss: 75.92722\n",
      "Epoch: 060, Loss: 74.42000\n",
      "Epoch: 061, Loss: 59.20856\n",
      "Epoch: 062, Loss: 74.33521\n",
      "Epoch: 063, Loss: 39.97620\n",
      "Epoch: 064, Loss: 72.05569\n",
      "Epoch: 065, Loss: 54.83687\n",
      "Epoch: 066, Loss: 56.06565\n",
      "Epoch: 067, Loss: 27.02633\n",
      "Epoch: 068, Loss: 35.35080\n",
      "Epoch: 069, Loss: 50.44155\n",
      "Epoch: 070, Loss: 34.06506\n",
      "Epoch: 071, Loss: 31.96500\n",
      "Epoch: 072, Loss: 58.12161\n",
      "Epoch: 073, Loss: 35.51534\n",
      "Epoch: 074, Loss: 57.50804\n",
      "Epoch: 075, Loss: 58.86952\n",
      "Epoch: 076, Loss: 53.09176\n",
      "Epoch: 077, Loss: 52.90013\n",
      "Epoch: 078, Loss: 68.23052\n",
      "Epoch: 079, Loss: 65.15421\n",
      "Epoch: 080, Loss: 42.31822\n",
      "Epoch: 081, Loss: 43.14433\n",
      "Epoch: 082, Loss: 25.38259\n",
      "Epoch: 083, Loss: 58.57185\n",
      "Epoch: 084, Loss: 41.35328\n",
      "Epoch: 085, Loss: 43.27484\n",
      "Epoch: 086, Loss: 60.05510\n",
      "Epoch: 087, Loss: 53.15888\n",
      "Epoch: 088, Loss: 20.69768\n",
      "Epoch: 089, Loss: 38.06441\n",
      "Epoch: 090, Loss: 35.57488\n",
      "Epoch: 091, Loss: 44.96572\n",
      "Epoch: 092, Loss: 44.27449\n",
      "Epoch: 093, Loss: 59.14566\n",
      "Epoch: 094, Loss: 33.66328\n",
      "Epoch: 095, Loss: 41.41347\n",
      "Epoch: 096, Loss: 45.48140\n",
      "Epoch: 097, Loss: 26.96906\n",
      "Epoch: 098, Loss: 36.90234\n",
      "Epoch: 099, Loss: 33.35231\n",
      "Epoch: 100, Loss: 16.37227\n",
      "Epoch: 101, Loss: 28.73783\n",
      "Epoch: 102, Loss: 21.51604\n",
      "Epoch: 103, Loss: 13.84731\n",
      "Epoch: 104, Loss: 14.35657\n",
      "Epoch: 105, Loss: 10.37675\n",
      "Epoch: 106, Loss: 21.19315\n",
      "Epoch: 107, Loss: 27.80610\n",
      "Epoch: 108, Loss: 25.52317\n",
      "Epoch: 109, Loss: 15.14220\n",
      "Epoch: 110, Loss: 22.28767\n",
      "Epoch: 111, Loss: 18.03925\n",
      "Epoch: 112, Loss: 27.37147\n",
      "Epoch: 113, Loss: 27.17728\n",
      "Epoch: 114, Loss: 16.55249\n",
      "Epoch: 115, Loss: 21.69882\n",
      "Epoch: 116, Loss: 27.65780\n",
      "Epoch: 117, Loss: 22.49927\n",
      "Epoch: 118, Loss: 20.69062\n",
      "Epoch: 119, Loss: 29.01280\n",
      "Epoch: 120, Loss: 23.81512\n",
      "Epoch: 121, Loss: 20.37695\n",
      "Epoch: 122, Loss: 8.94476\n",
      "Epoch: 123, Loss: 13.18580\n",
      "Epoch: 124, Loss: 12.54419\n",
      "Epoch: 125, Loss: 17.60903\n",
      "Epoch: 126, Loss: 15.28760\n",
      "Epoch: 127, Loss: 14.97581\n",
      "Epoch: 128, Loss: 13.97688\n",
      "Epoch: 129, Loss: 10.46105\n",
      "Epoch: 130, Loss: 12.28790\n",
      "Epoch: 131, Loss: 11.23037\n",
      "Epoch: 132, Loss: 7.07279\n",
      "Epoch: 133, Loss: 12.76178\n",
      "Epoch: 134, Loss: 9.51608\n",
      "Epoch: 135, Loss: 10.37348\n",
      "Epoch: 136, Loss: 11.94236\n",
      "Epoch: 137, Loss: 10.93979\n",
      "Epoch: 138, Loss: 8.84930\n",
      "Epoch: 139, Loss: 8.08072\n",
      "Epoch: 140, Loss: 8.76558\n",
      "Epoch: 141, Loss: 6.40433\n",
      "Epoch: 142, Loss: 13.21692\n",
      "Epoch: 143, Loss: 13.97510\n",
      "Epoch: 144, Loss: 9.91509\n",
      "Epoch: 145, Loss: 8.25716\n",
      "Epoch: 146, Loss: 7.78066\n",
      "Epoch: 147, Loss: 8.98852\n",
      "Epoch: 148, Loss: 9.32940\n",
      "Epoch: 149, Loss: 8.63990\n",
      "Epoch: 150, Loss: 12.29882\n",
      "Epoch: 151, Loss: 11.59360\n",
      "Epoch: 152, Loss: 11.15554\n",
      "Epoch: 153, Loss: 8.08828\n",
      "Epoch: 154, Loss: 13.69224\n",
      "Epoch: 155, Loss: 6.85050\n",
      "Epoch: 156, Loss: 7.60523\n",
      "Epoch: 157, Loss: 10.00265\n",
      "Epoch: 158, Loss: 7.40894\n",
      "Epoch: 159, Loss: 3.46077\n",
      "Epoch: 160, Loss: 5.21800\n",
      "Epoch: 161, Loss: 5.24322\n",
      "Epoch: 162, Loss: 4.40845\n",
      "Epoch: 163, Loss: 5.19217\n",
      "Epoch: 164, Loss: 5.02047\n",
      "Epoch: 165, Loss: 5.74892\n",
      "Epoch: 166, Loss: 5.40346\n",
      "Epoch: 167, Loss: 2.89512\n",
      "Epoch: 168, Loss: 4.36014\n",
      "Epoch: 169, Loss: 4.76262\n",
      "Epoch: 170, Loss: 7.30981\n",
      "Epoch: 171, Loss: 5.36450\n",
      "Epoch: 172, Loss: 4.72593\n",
      "Epoch: 173, Loss: 3.25143\n",
      "Epoch: 174, Loss: 3.28630\n",
      "Epoch: 175, Loss: 4.13231\n",
      "Epoch: 176, Loss: 4.19002\n",
      "Epoch: 177, Loss: 3.94382\n",
      "Epoch: 178, Loss: 5.60101\n",
      "Epoch: 179, Loss: 4.86735\n",
      "Epoch: 180, Loss: 5.17945\n",
      "Epoch: 181, Loss: 3.69485\n",
      "Epoch: 182, Loss: 3.68872\n",
      "Epoch: 183, Loss: 4.51224\n",
      "Epoch: 184, Loss: 4.80204\n",
      "Epoch: 185, Loss: 3.93823\n",
      "Epoch: 186, Loss: 5.14358\n",
      "Epoch: 187, Loss: 4.16430\n",
      "Epoch: 188, Loss: 5.27246\n",
      "Epoch: 189, Loss: 4.14360\n",
      "Epoch: 190, Loss: 4.03082\n",
      "Epoch: 191, Loss: 5.38611\n",
      "Epoch: 192, Loss: 4.64031\n",
      "Epoch: 193, Loss: 4.42036\n",
      "Epoch: 194, Loss: 4.04796\n",
      "Epoch: 195, Loss: 2.93287\n",
      "Epoch: 196, Loss: 2.57855\n",
      "Epoch: 197, Loss: 2.94473\n",
      "Epoch: 198, Loss: 2.59331\n",
      "Epoch: 199, Loss: 3.92823\n",
      "Accuracy: 20.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alishakhan/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "room_classes_names = [\"Background\", \"Outdoor\", \"Wall\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\", \"Entry\", \"Railing\", \"Storage\", \"Garage\"]\n",
    "\n",
    "# Define a function to create a PyTorch Geometric Data object from the room and edge data for each floorplan\n",
    "def create_data(attributes_df, edges_df):\n",
    "    edge_index = edges_df[['source', 'target']].values.T\n",
    "    x = attributes_df[['Area', 'Relative Area', 'Number of neighboring rooms']].values\n",
    "    y = attributes_df['Room Type'].values\n",
    "    return Data(x=torch.tensor(x, dtype=torch.float), edge_index=torch.tensor(edge_index, dtype=torch.long), y=torch.tensor(y, dtype=torch.long))\n",
    "\n",
    "# Create a list of Data objects, one for each floorplan\n",
    "data_list = []\n",
    "for i in range(len(attributes_df_list)):\n",
    "    attributes_df_i = attributes_df_list[i]\n",
    "    edges_df_i = edges_df_list[i]\n",
    "    data_list.append(create_data(attributes_df_i, edges_df_i))\n",
    "\n",
    "# Define the GCN architecture\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Create the GCN and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(num_features=3, num_classes=len(room_classes_names)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, optimizer, data_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        loss = F.nll_loss(out, data.y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(data_loader.dataset)\n",
    "\n",
    "# Train the GCN on your floorplan data\n",
    "num_classes = len(room_classes_names)\n",
    "model = Net(num_features=3, num_classes=len(room_classes_names)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "data_loader = DataLoader(data_list, batch_size=32, shuffle=True)\n",
    "for epoch in range(200):\n",
    "    loss = train(model, optimizer, data_loader)\n",
    "    print('Epoch: {:03d}, Loss: {:.5f}'.format(epoch, loss))\n",
    "\n",
    "# Define a function to create a PyTorch Geometric Data object from the room and edge data for each floorplan\n",
    "def create_data(attributes_df, edges_df):\n",
    "    edge_index = edges_df[['source', 'target']].values.T\n",
    "    x = attributes_df[['Area', 'Relative Area', 'Number of neighboring rooms']].values\n",
    "    return Data(x=torch.tensor(x, dtype=torch.float), edge_index=torch.tensor(edge_index, dtype=torch.long), y=None)\n",
    "\n",
    "# Create a list of Data objects for the test floorplans\n",
    "test_data_list = []\n",
    "for i in range(len(test_attributes_df_list)):\n",
    "    attributes_df_i = test_attributes_df_list[i].drop('Room Type', axis=1)\n",
    "    edges_df_i = test_edges_df_list[i]\n",
    "    test_data_list.append(create_data(attributes_df_i, edges_df_i))\n",
    "\n",
    "# Use the trained model to predict room types for the test data\n",
    "test_data_loader = DataLoader(test_data_list, batch_size=32, shuffle=False)\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for data in test_data_loader:\n",
    "        logits = model(data.x.to(device), data.edge_index.to(device))\n",
    "        pred = logits.argmax(dim=1)\n",
    "        pred_room_types = [room_classes_names[i] for i in pred.tolist()]\n",
    "        # Create a list of None values with the same length as the number of nodes in the graph\n",
    "        pred_room_types = pred_room_types + [None]*(len(data.x) - len(pred_room_types))\n",
    "        predictions.extend(pred_room_types)\n",
    "\n",
    "# Create a list of actual room types for the test data\n",
    "actual_room_types = []\n",
    "for attributes_df_i in test_attributes_df_list:\n",
    "    actual_room_types.extend(attributes_df_i['Room Type'].tolist())\n",
    "\n",
    "# Compare the actual and predicted room types and calculate accuracy\n",
    "correct_predictions = 0\n",
    "for i in range(len(actual_room_types)):\n",
    "    if actual_room_types[i] == predictions[i]:\n",
    "        correct_predictions += 1\n",
    "accuracy = correct_predictions / len(actual_room_types)\n",
    "\n",
    "print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e542035e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Kitchen       0.00      0.00      0.00       137\n",
      " Living Room       0.27      0.18      0.21       131\n",
      "    Bed Room       0.00      0.00      0.00       217\n",
      "        Bath       0.44      0.15      0.23       291\n",
      "       Entry       0.17      0.80      0.28       174\n",
      "     Storage       0.00      0.00      0.00        68\n",
      "      Garage       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.20      1020\n",
      "   macro avg       0.13      0.16      0.10      1020\n",
      "weighted avg       0.19      0.20      0.14      1020\n",
      "\n",
      "[[  0  16   0   6 115   0   0]\n",
      " [  0  23   0   3 105   0   0]\n",
      " [  0  17   0  13 187   0   0]\n",
      " [  0  16   0  45 230   0   0]\n",
      " [  0  10   0  24 140   0   0]\n",
      " [  0   2   0  11  55   0   0]\n",
      " [  0   0   0   0   2   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alishakhan/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/alishakhan/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/alishakhan/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Convert the room type labels from strings to integers\n",
    "room_classes_int = {name: i for i, name in enumerate(room_classes_names)}\n",
    "actual_room_types_int = [room_classes_int[r] for r in actual_room_types]\n",
    "predicted_room_types_int = [room_classes_int[r] for r in predictions]\n",
    "\n",
    "# Get the unique classes in the actual and predicted labels\n",
    "unique_classes = list(set(actual_room_types_int + predicted_room_types_int))\n",
    "\n",
    "# Convert the integer labels back to string labels\n",
    "room_classes_names_int = {i: name for name, i in room_classes_int.items()}\n",
    "target_names = [room_classes_names_int[i] for i in unique_classes]\n",
    "\n",
    "# Calculate precision, recall, F1 score, and confusion matrix\n",
    "print(classification_report(actual_room_types_int, predicted_room_types_int, labels=unique_classes, target_names=target_names))\n",
    "print(confusion_matrix(actual_room_types_int, predicted_room_types_int, labels=unique_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4601d9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
