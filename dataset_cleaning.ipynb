{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62484b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alishakhan/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/pandas/compat/_optional.py:149: UserWarning: Pandas requires version '1.3.1' or newer of 'bottleneck' (version '1.2.1' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a96786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation Split\n",
    "with open('/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/feature_extraction/train_df.pickle', 'rb') as f:\n",
    "    train_df = pickle.load(f)\n",
    "    \n",
    "with open('/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/feature_extraction/val_df.pickle', 'rb') as f:\n",
    "    val_df = pickle.load(f)\n",
    "    \n",
    "# CLEANING DATASETS we dont care about wall(2), background(0), outdoor(1), garage (10), undefined (11), railing (8)\n",
    "train_df = train_df[~train_df['Room'].isin([0, 1, 2, 10, 11, 8])]\n",
    "train_df = train_df.loc[:, train_df.columns != 'No Icon']\n",
    "\n",
    "# CLEANING DATASETS we dont care about wall(2), background(0), outdoor(1), garage (10), undefined (11), railing (8)\n",
    "val_df = val_df[~val_df['Room'].isin([0, 1, 2, 10, 11, 8])]\n",
    "val_df = val_df.loc[:, val_df.columns != 'No Icon']\n",
    "\n",
    "\n",
    "X_train = train_df[['Window', 'Door', 'Closet', 'Electrical Applience', 'Toilet', 'Sink', 'Sauna Bench', 'Fire Place', 'Bathtub', 'Chimney']]\n",
    "y_train = train_df['Room']\n",
    "\n",
    "X_val = val_df[['Window', 'Door', 'Closet', 'Electrical Applience', 'Toilet', 'Sink', 'Sauna Bench', 'Fire Place', 'Bathtub', 'Chimney']]\n",
    "y_val = val_df['Room']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef6c9ec",
   "metadata": {},
   "source": [
    "# Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d11fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.82      0.96      0.88       358\n",
      "           4       0.70      0.25      0.36       337\n",
      "           5       0.36      0.96      0.53       632\n",
      "           6       1.00      0.81      0.89       556\n",
      "           7       0.43      0.04      0.08       450\n",
      "           9       0.71      0.03      0.05       382\n",
      "\n",
      "    accuracy                           0.56      2715\n",
      "   macro avg       0.67      0.51      0.47      2715\n",
      "weighted avg       0.66      0.56      0.49      2715\n",
      "\n",
      "Accuracy: 0.5576427255985267\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Define the MNB classifier model\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Train the model on the training set\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable on the val set\n",
    "y_pred = mnb.predict(X_val)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Compute the accuracy score\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d0b815f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: 0.9010204081632653\n",
      "Threshold: 0.5\n",
      "Accuracy for threshold >= 0.5: 0.9010204081632653\n",
      "Accuracy for threshold < 0.5: 0.3636887608069164\n"
     ]
    }
   ],
   "source": [
    "# Get the predicted probabilities on the validation set\n",
    "probs = pd.DataFrame(mnb.predict_proba(X_val), columns=mnb.classes_)\n",
    "\n",
    "# Combine the actual and predicted values with the probabilities\n",
    "mnb_predictions = pd.DataFrame({'Actual': y_val, 'Predicted': y_pred}).reset_index(drop=True)\n",
    "probs = pd.concat([mnb_predictions, probs], axis=1)\n",
    "\n",
    "# Get the maximum probability for each instance\n",
    "probs_max = probs.iloc[:, 2:].max(axis=1)\n",
    "probs_max = pd.concat([probs.iloc[:, 0:2], probs_max], axis=1)\n",
    "probs_max.columns.values[2] = \"probability\"\n",
    "\n",
    "# Vary the probability threshold and compute the resulting accuracy\n",
    "for i in np.arange(0, 1, 0.01):\n",
    "    all_rows = probs_max[probs_max['probability'] >= i]\n",
    "    correct = all_rows.query('Actual == Predicted')\n",
    "    ratio = len(correct) / len(all_rows)\n",
    "    if ratio >= 0.9:\n",
    "        print(\"Ratio:\", ratio)\n",
    "        print(\"Threshold:\", i)\n",
    "        break\n",
    "\n",
    "# Compute the accuracy for the threshold of >=i\n",
    "accuracy_high = len(probs_max.query(f'probability >= {i}').query('Actual == Predicted')) / len(probs_max.query(f'probability >= {i}'))\n",
    "print(f\"Accuracy for threshold >= {i}:\", accuracy_high)\n",
    "\n",
    "# Compute the accuracy for the threshold of <i\n",
    "accuracy_low = len(probs_max.query(f'probability < {i}').query('Actual == Predicted')) / len(probs_max.query(f'probability < {i}'))\n",
    "print(f\"Accuracy for threshold < {i}:\", accuracy_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "771596af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.96      0.93      0.94       358\n",
      "           4       0.34      0.88      0.49       337\n",
      "           5       0.51      0.64      0.57       632\n",
      "           6       0.92      0.85      0.89       556\n",
      "           7       0.45      0.16      0.24       450\n",
      "           9       0.36      0.01      0.02       382\n",
      "\n",
      "    accuracy                           0.59      2715\n",
      "   macro avg       0.59      0.58      0.52      2715\n",
      "weighted avg       0.60      0.59      0.54      2715\n",
      "\n",
      "Accuracy: 0.5860036832412523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the random forest classifier model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Train the model on the training set\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable on the validation set\n",
    "y_pred = rfc.predict(X_val)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Compute the accuracy score\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d76fa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.96      0.93      0.95       358\n",
      "           4       0.81      0.24      0.37       337\n",
      "           5       0.50      0.66      0.57       632\n",
      "           6       0.99      0.83      0.90       556\n",
      "           7       0.42      0.17      0.24       450\n",
      "           9       0.24      0.51      0.33       382\n",
      "\n",
      "    accuracy                           0.57      2715\n",
      "   macro avg       0.65      0.56      0.56      2715\n",
      "weighted avg       0.65      0.57      0.57      2715\n",
      "\n",
      "Accuracy: 0.574585635359116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alishakhan/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the k-nearest neighbors classifier model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Train the model on the training set\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable on the testing set\n",
    "y_pred = knn.predict(X_val)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Compute the accuracy score\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1b7a941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.96      0.93      0.95       358\n",
      "           4       0.62      0.28      0.38       337\n",
      "           5       0.37      0.96      0.53       632\n",
      "           6       0.97      0.83      0.89       556\n",
      "           7       0.49      0.08      0.13       450\n",
      "           9       0.50      0.00      0.01       382\n",
      "\n",
      "    accuracy                           0.57      2715\n",
      "   macro avg       0.65      0.51      0.48      2715\n",
      "weighted avg       0.64      0.57      0.50      2715\n",
      "\n",
      "Accuracy: 0.565377532228361\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the logistic regression classifier model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model on the training set\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable on the testing set\n",
    "y_pred = lr.predict(X_val)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Compute the accuracy score\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e154296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: 0.9323770491803278\n",
      "Threshold: 0.46\n",
      "Accuracy for threshold >= 0.46: 0.9323770491803278\n",
      "Accuracy for threshold < 0.46: 0.359401955146636\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities for each class on the validation set\n",
    "probs = pd.DataFrame(lr.predict_proba(X_val), columns=lr.classes_)\n",
    "\n",
    "# Combine the actual and predicted values with the probabilities\n",
    "lr_predictions = pd.DataFrame({'Actual': y_val, 'Predicted': lr.predict(X_val)}).reset_index(drop=True)\n",
    "probs = pd.concat([lr_predictions, probs], axis=1)\n",
    "\n",
    "# Get the maximum probability for each instance\n",
    "probs_max = probs.iloc[:, 2:].max(axis=1)\n",
    "probs_max = pd.concat([probs.iloc[:, 0:2], probs_max], axis=1)\n",
    "probs_max.columns.values[2] = \"probability\"\n",
    "\n",
    "# Vary the probability threshold and compute the resulting accuracy\n",
    "for i in np.arange(0, 1, 0.01):\n",
    "    all_rows = probs_max[probs_max['probability'] >= i]\n",
    "    correct = all_rows.query('Actual == Predicted')\n",
    "    ratio = len(correct) / len(all_rows)\n",
    "    if ratio >= 0.9:\n",
    "        print(\"Ratio:\", ratio)\n",
    "        print(\"Threshold:\", i)\n",
    "        break\n",
    "\n",
    "# Compute the accuracy for the threshold of >=i\n",
    "accuracy_high = len(probs_max.query(f'probability >= {i}').query('Actual == Predicted')) / len(probs_max.query(f'probability >= {i}'))\n",
    "print(f\"Accuracy for threshold >= {i}:\", accuracy_high)\n",
    "\n",
    "# Compute the accuracy for the threshold of <i\n",
    "accuracy_low = len(probs_max.query(f'probability < {i}').query('Actual == Predicted')) / len(probs_max.query(f'probability < {i}'))\n",
    "print(f\"Accuracy for threshold < {i}:\", accuracy_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cd59fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'lr_classification_model.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(lr, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5504a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
