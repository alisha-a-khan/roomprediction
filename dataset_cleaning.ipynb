{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb85475b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alishakhan/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/pandas/compat/_optional.py:149: UserWarning: Pandas requires version '1.3.1' or newer of 'bottleneck' (version '1.2.1' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5bd2517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation Split\n",
    "with open('/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/feature_extraction/train_df.pickle', 'rb') as f:\n",
    "    train_df = pickle.load(f)\n",
    "    \n",
    "with open('/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/feature_extraction/val_df.pickle', 'rb') as f:\n",
    "    val_df = pickle.load(f)\n",
    "    \n",
    "with open('/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/feature_extraction/test_df.pickle', 'rb') as f:\n",
    "    test_df = pickle.load(f)\n",
    "    \n",
    "# CLEANING DATASETS we dont care about wall(2), background(0), outdoor(1), garage (10), undefined (11), railing (8)\n",
    "train_df = train_df[~train_df['Room'].isin([0, 1, 2, 10, 11, 8])]\n",
    "train_df = train_df.loc[:, train_df.columns != 'No Icon']\n",
    "\n",
    "# CLEANING DATASETS we dont care about wall(2), background(0), outdoor(1), garage (10), undefined (11), railing (8)\n",
    "val_df = val_df[~val_df['Room'].isin([0, 1, 2, 10, 11, 8])]\n",
    "val_df = val_df.loc[:, val_df.columns != 'No Icon']\n",
    "\n",
    "# CLEANING DATASETS we dont care about wall(2), background(0), outdoor(1), garage (10), undefined (11), railing (8)\n",
    "test_df = test_df[~test_df['Room'].isin([0, 1, 2, 10, 11, 8])]\n",
    "test_df = test_df.loc[:, test_df.columns != 'No Icon']\n",
    "\n",
    "X_train = train_df[['Window', 'Door', 'Closet', 'Electrical Applience', 'Toilet', 'Sink', 'Sauna Bench', 'Fire Place', 'Bathtub', 'Chimney']]\n",
    "y_train = train_df['Room']\n",
    "\n",
    "X_val = val_df[['Window', 'Door', 'Closet', 'Electrical Applience', 'Toilet', 'Sink', 'Sauna Bench', 'Fire Place', 'Bathtub', 'Chimney']]\n",
    "y_val = val_df['Room']\n",
    "\n",
    "X_test = test_df[['Window', 'Door', 'Closet', 'Electrical Applience', 'Toilet', 'Sink', 'Sauna Bench', 'Fire Place', 'Bathtub', 'Chimney']]\n",
    "y_test = test_df['Room']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920b9dc3",
   "metadata": {},
   "source": [
    "# Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1768f718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.82      0.96      0.88       358\n",
      "           4       0.70      0.25      0.36       337\n",
      "           5       0.36      0.96      0.53       632\n",
      "           6       1.00      0.81      0.89       556\n",
      "           7       0.43      0.04      0.08       450\n",
      "           9       0.71      0.03      0.05       382\n",
      "\n",
      "    accuracy                           0.56      2715\n",
      "   macro avg       0.67      0.51      0.47      2715\n",
      "weighted avg       0.66      0.56      0.49      2715\n",
      "\n",
      "Accuracy: 0.5576427255985267\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Define the MNB classifier model\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Train the model on the training set\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable on the val set\n",
    "y_pred = mnb.predict(X_val)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Compute the accuracy score\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa9e495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: 0.9010204081632653\n",
      "Threshold: 0.5\n",
      "Accuracy for threshold >= 0.5: 0.9010204081632653\n",
      "Accuracy for threshold < 0.5: 0.3636887608069164\n"
     ]
    }
   ],
   "source": [
    "# Get the predicted probabilities on the validation set\n",
    "probs = pd.DataFrame(mnb.predict_proba(X_val), columns=mnb.classes_)\n",
    "\n",
    "# Combine the actual and predicted values with the probabilities\n",
    "mnb_predictions = pd.DataFrame({'Actual': y_val, 'Predicted': y_pred}).reset_index(drop=True)\n",
    "probs = pd.concat([mnb_predictions, probs], axis=1)\n",
    "\n",
    "# Get the maximum probability for each instance\n",
    "probs_max = probs.iloc[:, 2:].max(axis=1)\n",
    "probs_max = pd.concat([probs.iloc[:, 0:2], probs_max], axis=1)\n",
    "probs_max.columns.values[2] = \"probability\"\n",
    "\n",
    "# Vary the probability threshold and compute the resulting accuracy\n",
    "for i in np.arange(0, 1, 0.01):\n",
    "    all_rows = probs_max[probs_max['probability'] >= i]\n",
    "    correct = all_rows.query('Actual == Predicted')\n",
    "    ratio = len(correct) / len(all_rows)\n",
    "    if ratio >= 0.9:\n",
    "        print(\"Ratio:\", ratio)\n",
    "        print(\"Threshold:\", i)\n",
    "        break\n",
    "\n",
    "# Compute the accuracy for the threshold of >=i\n",
    "accuracy_high = len(probs_max.query(f'probability >= {i}').query('Actual == Predicted')) / len(probs_max.query(f'probability >= {i}'))\n",
    "print(f\"Accuracy for threshold >= {i}:\", accuracy_high)\n",
    "\n",
    "# Compute the accuracy for the threshold of <i\n",
    "accuracy_low = len(probs_max.query(f'probability < {i}').query('Actual == Predicted')) / len(probs_max.query(f'probability < {i}'))\n",
    "print(f\"Accuracy for threshold < {i}:\", accuracy_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9d2877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.96      0.93      0.94       358\n",
      "           4       0.34      0.88      0.49       337\n",
      "           5       0.51      0.64      0.57       632\n",
      "           6       0.92      0.86      0.89       556\n",
      "           7       0.45      0.16      0.24       450\n",
      "           9       0.40      0.01      0.02       382\n",
      "\n",
      "    accuracy                           0.59      2715\n",
      "   macro avg       0.60      0.58      0.53      2715\n",
      "weighted avg       0.61      0.59      0.54      2715\n",
      "\n",
      "Accuracy: 0.5863720073664825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the random forest classifier model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Train the model on the training set\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable on the validation set\n",
    "y_pred = rfc.predict(X_val)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Compute the accuracy score\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65e3a4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.96      0.93      0.95       358\n",
      "           4       0.81      0.24      0.37       337\n",
      "           5       0.50      0.66      0.57       632\n",
      "           6       0.99      0.83      0.90       556\n",
      "           7       0.42      0.17      0.24       450\n",
      "           9       0.24      0.51      0.33       382\n",
      "\n",
      "    accuracy                           0.57      2715\n",
      "   macro avg       0.65      0.56      0.56      2715\n",
      "weighted avg       0.65      0.57      0.57      2715\n",
      "\n",
      "Accuracy: 0.574585635359116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alishakhan/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the k-nearest neighbors classifier model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Train the model on the training set\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable on the testing set\n",
    "y_pred = knn.predict(X_val)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Compute the accuracy score\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4c5c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.96      0.93      0.95       358\n",
      "           4       0.62      0.28      0.38       337\n",
      "           5       0.37      0.96      0.53       632\n",
      "           6       0.97      0.83      0.89       556\n",
      "           7       0.49      0.08      0.13       450\n",
      "           9       0.50      0.00      0.01       382\n",
      "\n",
      "    accuracy                           0.57      2715\n",
      "   macro avg       0.65      0.51      0.48      2715\n",
      "weighted avg       0.64      0.57      0.50      2715\n",
      "\n",
      "Accuracy: 0.565377532228361\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the logistic regression classifier model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model on the training set\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable on the testing set\n",
    "y_pred = lr.predict(X_val)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Compute the accuracy score\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7f84322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: 0.9323770491803278\n",
      "Threshold: 0.46\n",
      "Accuracy for threshold >= 0.46: 0.9323770491803278\n",
      "Accuracy for threshold < 0.46: 0.359401955146636\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities for each class on the validation set\n",
    "probs = pd.DataFrame(lr.predict_proba(X_val), columns=lr.classes_)\n",
    "\n",
    "# Combine the actual and predicted values with the probabilities\n",
    "lr_predictions = pd.DataFrame({'Actual': y_val, 'Predicted': lr.predict(X_val)}).reset_index(drop=True)\n",
    "probs = pd.concat([lr_predictions, probs], axis=1)\n",
    "\n",
    "# Get the maximum probability for each instance\n",
    "probs_max = probs.iloc[:, 2:].max(axis=1)\n",
    "probs_max = pd.concat([probs.iloc[:, 0:2], probs_max], axis=1)\n",
    "probs_max.columns.values[2] = \"probability\"\n",
    "\n",
    "# Vary the probability threshold and compute the resulting accuracy\n",
    "for i in np.arange(0, 1, 0.01):\n",
    "    all_rows = probs_max[probs_max['probability'] >= i]\n",
    "    correct = all_rows.query('Actual == Predicted')\n",
    "    ratio = len(correct) / len(all_rows)\n",
    "    if ratio >= 0.9:\n",
    "        print(\"Ratio:\", ratio)\n",
    "        print(\"Threshold:\", i)\n",
    "        break\n",
    "\n",
    "# Compute the accuracy for the threshold of >=i\n",
    "accuracy_high = len(probs_max.query(f'probability >= {i}').query('Actual == Predicted')) / len(probs_max.query(f'probability >= {i}'))\n",
    "print(f\"Accuracy for threshold >= {i}:\", accuracy_high)\n",
    "\n",
    "# Compute the accuracy for the threshold of <i\n",
    "accuracy_low = len(probs_max.query(f'probability < {i}').query('Actual == Predicted')) / len(probs_max.query(f'probability < {i}'))\n",
    "print(f\"Accuracy for threshold < {i}:\", accuracy_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2071e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'lr_classification_model.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(lr, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64fa81b",
   "metadata": {},
   "source": [
    "# Modifying Floorplans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4177551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of pickle files\n",
    "files = ['/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/feature_extraction/train_df.pickle', \n",
    "         '/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/feature_extraction/test_df.pickle', \n",
    "         '/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/feature_extraction/val_df.pickle']\n",
    "\n",
    "# Load the pickle files into a list of dataframes\n",
    "dfs = []\n",
    "for file in files:\n",
    "    with open(file, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "        dfs.append(df)\n",
    "\n",
    "# Combine the dataframes into a single dataframe\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "# Map the values in the 'Room' column to the room classes\n",
    "room_classes = [\"Background\", \"Outdoor\", \"Wall\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\", \"Entry\", \"Railing\", \"Storage\", \"Garage\", \"Undefined\"]\n",
    "df['Room'] = df['Room'].map(dict(enumerate(room_classes)))\n",
    "\n",
    "# Add a 'Set' column to the dataframe\n",
    "set_names = ['Train'] * len(dfs[0]) + ['Test'] * len(dfs[1]) + ['Validation'] * len(dfs[2])\n",
    "df['Set'] = set_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195bd573",
   "metadata": {},
   "source": [
    "## Manually rules for undefined rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef21ccdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of undefined rooms: 15750\n",
      "Number of undefined rooms with a toilet or sauna bench: 4810\n",
      "Number of undefined rooms after modification: 10940\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of undefined rooms: {len(df[df['Room'] == 'Undefined'])}\")\n",
    "\n",
    "# Count the number of rooms that are undefined and contain a toilet or sauna bench\n",
    "num_undefined_toilet_sauna = len(df[(df['Room'] == 'Undefined') & ((df['Toilet']>0) | (df['Sauna Bench']>0))])\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of undefined rooms with a toilet or sauna bench: {num_undefined_toilet_sauna}\")\n",
    "\n",
    "\n",
    "# Set the 'Room' value for rows that are undefined and have a toilet or sauna bench to 'Bath Room'\n",
    "df.loc[(df['Room'] == 'Undefined') & ((df['Toilet'] > 0) | (df['Sauna Bench'] > 0)), 'Room'] = 'Bath'\n",
    "\n",
    "\n",
    "print(f\"Number of undefined rooms after modification: {len(df[df['Room'] == 'Undefined'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d462eaeb",
   "metadata": {},
   "source": [
    "## Filling with the multiclass logistic regression and probability threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ab4f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values to exclude, because we dont want to predict these rooms. when we care about accuracy, we don't care about predicting rooms of these type to another type, that's why i remove it from val and test as well\n",
    "exclude = ['Wall', 'Background', 'Outdoor', 'Garage', 'Railing']\n",
    "\n",
    "# Create a new dataframe with rows where the 'Room' value is not in the exclude list\n",
    "df = df[~df['Room'].isin(exclude)]\n",
    "#remove no icon from the columns\n",
    "df = df.loc[:, df.columns != 'No Icon']\n",
    "\n",
    "# Get the rows with undefined rooms\n",
    "undefined_df=df[df['Room']=='Undefined']\n",
    "\n",
    "# Get the feature matrix for the undefined rooms\n",
    "X_undefined = undefined_df[['Window', 'Door', 'Closet', 'Electrical Applience', 'Toilet', 'Sink', 'Sauna Bench', 'Fire Place', 'Bathtub', 'Chimney']]\n",
    "\n",
    "# Predict the probabilities for each class for the undefined rooms\n",
    "probs_undefined = lr.predict_proba(X_undefined)\n",
    "\n",
    "# Get the predicted class for each row\n",
    "predicted_classes = lr.classes_[np.argmax(probs_undefined, axis=1)]\n",
    "\n",
    "# Get the maximum predicted probability for each row\n",
    "max_probs = np.max(probs_undefined, axis=1)\n",
    "\n",
    "# Replace the Room value for rows where the max probability is greater than or equal to the threshold\n",
    "undefined_copy = undefined_df.copy()\n",
    "undefined_copy.loc[max_probs >= i, 'Room'] = predicted_classes[max_probs >= i]\n",
    "undefined_copy.reset_index(drop=True, inplace=True) # reset the index\n",
    "\n",
    "# List of values to exclude\n",
    "exclude = ['Wall', 'Background', 'Outdoor', 'Garage', 'Railing']\n",
    "\n",
    "# Create a new dataframe with rows where the 'Room' value is not in the exclude list\n",
    "filtered_df = df[~df['Room'].isin(exclude)]\n",
    "filtered_df = filtered_df.loc[:, filtered_df.columns != 'No Icon']\n",
    "\n",
    "# Reset the index of filtered_df\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update the filtered_df with the updated Room values for the undefined rooms\n",
    "filtered_df.loc[undefined_copy.index, 'Room'] = undefined_copy['Room']\n",
    "\n",
    "room_classes = [\"Background\", \"Outdoor\", \"Wall\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\", \"Entry\", \"Railing\", \"Storage\", \"Garage\", \"Undefined\"]\n",
    "filtered_df['Room'].replace({3: room_classes[3], 4: room_classes[4], 5: room_classes[5], 6: room_classes[6], 7: room_classes[7]}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e34f46e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a2c3267",
   "metadata": {},
   "outputs": [],
   "source": [
    "room_model=lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12402c65",
   "metadata": {},
   "source": [
    "# Relabeling Undefined Rooms in Floorplans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81532e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Update PYTHONPATH to include the path to the floortrans folder\n",
    "os.environ['PYTHONPATH'] = '/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git:' + os.environ.get('PYTHONPATH', '')\n",
    "\n",
    "%matplotlib inline\n",
    "from skimage import transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from floortrans.models import get_model\n",
    "from floortrans.loaders import FloorplanSVG, DictToTensor, Compose, RotateNTurns\n",
    "from floortrans.plotting import segmentation_plot, polygons_to_image, draw_junction_from_dict, discrete_cmap\n",
    "from floortrans.plotting import discrete_cmap\n",
    "from floortrans.post_prosessing import split_prediction, get_polygons, split_validation\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "import cv2\n",
    "import random\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "rot = RotateNTurns() #\n",
    "room_classes = [\"Background\", \"Outdoor\", \"Wall\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\", \"Entry\", \"Railing\", \"Storage\", \"Garage\", \"Undefined\"]\n",
    "icon_classes = [\"No Icon\", \"Window\", \"Door\", \"Closet\", \"Electrical Applience\" ,\"Toilet\", \"Sink\", \"Sauna Bench\", \"Fire Place\", \"Bathtub\", \"Chimney\"]\n",
    "\n",
    "data_folder = '../data/cubicasa5k/'\n",
    "##### MODIFY THIS WHEN YOU RUN\n",
    "data_file = 'train.txt' \n",
    "normal_set = FloorplanSVG(data_folder, data_file, format='txt', original_size=True)\n",
    "data_loader = DataLoader(normal_set, batch_size=1, num_workers=0)\n",
    "data_iter = iter(data_loader)\n",
    "\n",
    "# Setup Model\n",
    "model = get_model('hg_furukawa_original', 51)\n",
    "\n",
    "n_classes = 44\n",
    "split = [21, 12, 11]\n",
    "\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dc488a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f051578d44e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnormal_set_copy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/floortrans/loaders/svg_loader.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/floortrans/loaders/svg_loader.py\u001b[0m in \u001b[0;36mget_txt\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Getting labels for segmentation and heatmaps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mhouse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHouse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvg_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Combining them to one numpy tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhouse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_segmentation_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/floortrans/loaders/house.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, height, width, icon_list, room_list)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_walls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_end_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpillar_walls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/floortrans/loaders/svg_utils.py\u001b[0m in \u001b[0;36mchange_end_points\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchange_end_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'V'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_coord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'H'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3430\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3432\u001b[0;31m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0m\u001b[1;32m   3433\u001b[0m                           out=out, **kwargs)\n\u001b[1;32m   3434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def isolate_class(rooms, CLASS: int):\n",
    "    template = np.zeros_like(rooms)\n",
    "    rows, cols = np.where(rooms == CLASS)\n",
    "    template[rows, cols] = 1\n",
    "    return template\n",
    "\n",
    "def relabel_class(inp, contours, original_class: int, target_class: int):\n",
    "    cv2.fillPoly(inp, pts=[contours], color=target_class)\n",
    "    return inp\n",
    "\n",
    "normal_set_copy=list(normal_set)\n",
    "print(done)\n",
    "for idx, val in enumerate(normal_set):\n",
    "    print(idx)\n",
    "    label = val['label']\n",
    "    label_np = label.data.numpy()\n",
    "    rooms=label_np[0].astype(np.uint8)\n",
    "    icons=label_np[1].astype(np.uint8)\n",
    "    modified_rooms=rooms.copy()\n",
    "    \n",
    "    \n",
    "    #step 1 isolate rooms\n",
    "    isolated=isolate_class(modified_rooms, 11)\n",
    "    \n",
    "    contours, _ = cv2.findContours(isolated.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) # find individual incidents of the class, RETR_EXTERNAL just means only consider external contours (no donuts)        \n",
    "   \n",
    "    #step 2 look at each undefined room\n",
    "    \n",
    "    for rc in contours:\n",
    "        rc=np.squeeze(rc,1) # removes the first dimension\n",
    "        if rc.shape[0]>=4:\n",
    "            \n",
    "            room_cont = Polygon(rc).buffer(0) # the .buffer(0) operation is the simpliest way to ensure that the polygon stays valid\n",
    "            x = np.zeros(len(icon_classes)) # initiate a new x incident\n",
    "            for icon in range(len(icon_classes)): # loop through all the icons\n",
    "                isolated_icon = isolate_class(icons, icon) # selects the region on the image where the class exists\n",
    "                icon_contours, _ = cv2.findContours(isolated_icon, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) # find indivudual incidents of the class\n",
    "                for ic in icon_contours: # loop through each individual incidents\n",
    "                    ic = np.squeeze(ic, 1) # removes the first dimension\n",
    "                    if ic.shape[0]>=4:\n",
    "                        #instead can take the average of the icon. and see if that point \n",
    "                        icon_cont = Polygon(ic).buffer(0) # again...\n",
    "                        if room_cont.intersects(icon_cont): # <- if they intersects then theres one incident of overlapping between the room type and icon type\n",
    "                            x[icon] += 1 # for this room type incident, there's one more incident of overlap with this icon class\n",
    "            \n",
    "            #step 3: predict the room\n",
    "            probabilities = room_model.predict_proba([x[1:]])[0]\n",
    "            probability = max(probabilities)\n",
    "            prediction = np.argmax(probabilities)\n",
    "            classes=room_model.classes_\n",
    "            prediction=classes[prediction]\n",
    "            prediction=room_classes[int(prediction)]\n",
    "            if probability<0.5:\n",
    "                prediction=\"Undefined\"\n",
    "            if x[6]+x[7]>0: #sauna or toilet then bath\n",
    "                prediction=\"Bath\"\n",
    "            if sum(x[1:])==0:\n",
    "                prediction=\"Undefined\"\n",
    "            \n",
    "            #step 4: relabel class\n",
    "            modified_rooms=relabel_class(modified_rooms, rc, 11, room_classes.index(prediction))\n",
    "    val['modified_floorplan']=modified_rooms\n",
    "    normal_set_copy[idx]=val\n",
    "    # Save the modified normal_set_copy to a file\n",
    "    with open('/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/modified_floorplans/train.pkl', 'wb') as file:\n",
    "        pickle.dump(normal_set_copy, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8504f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_file = open(\"/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/modified_floorplans/train.pkl\", \"wb\")\n",
    "pickle.dump(normal_set_copy, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f46eeaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
