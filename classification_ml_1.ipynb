{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alishakhan/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/pandas/compat/_optional.py:149: UserWarning: Pandas requires version '1.3.1' or newer of 'bottleneck' (version '1.2.1' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Datasets\n",
    "with open('/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/feature_extraction/train_df.pickle', 'rb') as f:\n",
    "    train_df = pickle.load(f)\n",
    "\n",
    "with open('/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/feature_extraction/test_df.pickle', 'rb') as f:\n",
    "    test_df = pickle.load(f)\n",
    "\n",
    "with open('/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/feature_extraction/val_df.pickle', 'rb') as f:\n",
    "    val_df = pickle.load(f)\n",
    "\n",
    "#combining test and val\n",
    "test_df = pd.concat([test_df, val_df])\n",
    "\n",
    "# CLEANING DATASETS we dont care about wall(2), background(0), outdoor(1), garage (10), undefined (11), railing (8)\n",
    "train_df = train_df[~train_df['Room'].isin([0, 1, 2, 10, 11, 8])]\n",
    "train_df = train_df.loc[:, train_df.columns != 'No Icon']\n",
    "\n",
    "test_df = test_df[~test_df['Room'].isin([0, 1, 2, 10, 11, 8])]\n",
    "test_df = test_df.loc[:, test_df.columns != 'No Icon']\n",
    "\n",
    "# Train test\n",
    "X_train = train_df[['Window', 'Door', 'Closet', 'Electrical Applience', 'Toilet', 'Sink', 'Sauna Bench', 'Fire Place', 'Bathtub', 'Chimney']]\n",
    "y_train = train_df['Room']\n",
    "\n",
    "X_test = test_df[['Window', 'Door', 'Closet', 'Electrical Applience', 'Toilet', 'Sink', 'Sauna Bench', 'Fire Place', 'Bathtub', 'Chimney']]\n",
    "y_test = test_df['Room']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.81      0.94      0.87       722\n",
      "           4       0.71      0.25      0.37       671\n",
      "           5       0.36      0.96      0.53      1257\n",
      "           6       0.99      0.81      0.89      1159\n",
      "           7       0.45      0.04      0.08       922\n",
      "           9       0.45      0.03      0.05       758\n",
      "\n",
      "    accuracy                           0.56      5489\n",
      "   macro avg       0.63      0.51      0.46      5489\n",
      "weighted avg       0.62      0.56      0.49      5489\n",
      "\n",
      "Accuracy: 0.5563854982692658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Define the MNB classifier model\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Train the model on the training set\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable on the testing set\n",
    "y_pred = mnb.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: 0.9002991026919243\n",
      "Threshold: 0.48\n",
      "Accuracy for threshold >= 0.48: 0.9002991026919243\n",
      "Accuracy for threshold < 0.48: 0.35831180017226527\n"
     ]
    }
   ],
   "source": [
    "# Get the predicted probabilities on the testing set\n",
    "probs = pd.DataFrame(mnb.predict_proba(X_test), columns=mnb.classes_)\n",
    "\n",
    "# Combine the actual and predicted values with the probabilities\n",
    "mnb_predictions = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}).reset_index(drop=True)\n",
    "probs = pd.concat([mnb_predictions, probs], axis=1)\n",
    "\n",
    "# Get the maximum probability for each instance\n",
    "probs_max = probs.iloc[:, 2:].max(axis=1)\n",
    "probs_max = pd.concat([probs.iloc[:, 0:2], probs_max], axis=1)\n",
    "probs_max.columns.values[2] = \"probability\"\n",
    "\n",
    "# Vary the probability threshold and compute the resulting accuracy\n",
    "for i in np.arange(0, 1, 0.01):\n",
    "    all_rows = probs_max[probs_max['probability'] >= i]\n",
    "    correct = all_rows.query('Actual == Predicted')\n",
    "    ratio = len(correct) / len(all_rows)\n",
    "    if ratio >= 0.9:\n",
    "        print(\"Ratio:\", ratio)\n",
    "        print(\"Threshold:\", i)\n",
    "        break\n",
    "\n",
    "# Compute the accuracy for the threshold of 0.51\n",
    "accuracy_high = len(probs_max.query(f'probability >= {i}').query('Actual == Predicted')) / len(probs_max.query(f'probability >= {i}'))\n",
    "print(f\"Accuracy for threshold >= {i}:\", accuracy_high)\n",
    "\n",
    "# Compute the accuracy for the threshold of 0.5\n",
    "accuracy_low = len(probs_max.query(f'probability < {i}').query('Actual == Predicted')) / len(probs_max.query(f'probability < {i}'))\n",
    "print(f\"Accuracy for threshold < {i}:\", accuracy_low)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'room_classification_model_1.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(mnb, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.95      0.91      0.93       722\n",
      "           4       0.33      0.88      0.47       671\n",
      "           5       0.51      0.63      0.56      1257\n",
      "           6       0.92      0.86      0.89      1159\n",
      "           7       0.44      0.15      0.23       922\n",
      "           9       0.39      0.02      0.03       758\n",
      "\n",
      "    accuracy                           0.58      5489\n",
      "   macro avg       0.59      0.57      0.52      5489\n",
      "weighted avg       0.60      0.58      0.54      5489\n",
      "\n",
      "Accuracy: 0.5797048642740026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the random forest classifier model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Train the model on the training set\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable on the testing set\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.95      0.91      0.93       722\n",
      "           4       0.81      0.24      0.37       671\n",
      "           5       0.50      0.64      0.56      1257\n",
      "           6       0.98      0.83      0.90      1159\n",
      "           7       0.41      0.16      0.23       922\n",
      "           9       0.24      0.52      0.33       758\n",
      "\n",
      "    accuracy                           0.57      5489\n",
      "   macro avg       0.65      0.55      0.55      5489\n",
      "weighted avg       0.65      0.57      0.57      5489\n",
      "\n",
      "Accuracy: 0.5689560940061942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alishakhan/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the k-nearest neighbors classifier model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Train the model on the training set\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable on the testing set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.96      0.91      0.93       722\n",
      "           4       0.65      0.28      0.39       671\n",
      "           5       0.36      0.97      0.53      1257\n",
      "           6       0.96      0.84      0.90      1159\n",
      "           7       0.52      0.09      0.15       922\n",
      "           9       0.50      0.01      0.01       758\n",
      "\n",
      "    accuracy                           0.57      5489\n",
      "   macro avg       0.66      0.52      0.49      5489\n",
      "weighted avg       0.65      0.57      0.51      5489\n",
      "\n",
      "Accuracy: 0.5689560940061942\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the logistic regression classifier model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model on the training set\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable on the testing set\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
