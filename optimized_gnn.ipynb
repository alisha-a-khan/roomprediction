{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4978e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alishakhan/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/pandas/compat/_optional.py:149: UserWarning: Pandas requires version '1.3.1' or newer of 'bottleneck' (version '1.2.1' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/alishakhan/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 8\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.5\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 16\n",
      "dropout rate 0.7\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.001\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.01\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.0005\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.3\n",
      "learning rate 0.1\n",
      "weight decay 0.001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 32\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0001\n",
      "batch size 64\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden units 32\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 16\n",
      "num epochs 300\n",
      "hidden units 32\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 100\n",
      "hidden units 32\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 200\n",
      "hidden units 32\n",
      "dropout rate 0.5\n",
      "learning rate 0.001\n",
      "weight decay 0.0005\n",
      "batch size 32\n",
      "num epochs 300\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open('/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/gnn_input/train_data.pkl', 'rb') as f:\n",
    "    embeddings, Y, df, X_all, edges_df_list, attributes_df_list = pickle.load(f)\n",
    "with open('/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git/submission/gnn_input/val_data.pkl', 'rb') as f:\n",
    "    val_embeddings, val_Y, val_df, val_X_all, val_edges_df_list, val_attributes_df_list = pickle.load(f)\n",
    "\n",
    "Y=Y.argmax(1)\n",
    "val_Y=val_Y.argmax(1)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "room_classes_names = [\"Background\", \"Outdoor\", \"Wall\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\", \"Entry\", \"Railing\", \"Storage\", \"Garage\"]\n",
    "\n",
    "for df in attributes_df_list:\n",
    "    df[\"Room Type\"] = df[\"Room Type\"].apply(lambda x: room_classes_names.index(x))\n",
    "\n",
    "# Define a function to create a PyTorch Geometric Data object from the room and edge data for each floorplan\n",
    "def create_data(attributes_df, edges_df):\n",
    "    edge_index = edges_df[['source', 'target']].values.T\n",
    "    x = attributes_df[['Area', 'Relative Area', 'Number of neighboring rooms']].values\n",
    "    y = attributes_df['Room Type'].values\n",
    "    return Data(x=torch.tensor(x, dtype=torch.float), edge_index=torch.tensor(edge_index, dtype=torch.long), y=torch.tensor(y, dtype=torch.long))\n",
    "\n",
    "# Create a list of Data objects, one for each floorplan\n",
    "data_list = []\n",
    "for i in range(len(attributes_df_list)):\n",
    "    attributes_df_i = attributes_df_list[i]\n",
    "    edges_df_i = edges_df_list[i]\n",
    "    data_list.append(create_data(attributes_df_i, edges_df_i))\n",
    "\n",
    "# Define a function to create a PyTorch Geometric Data object from the room and edge data for each floorplan\n",
    "def create_data(attributes_df, edges_df):\n",
    "    edge_index = edges_df[['source', 'target']].values.T\n",
    "    x = attributes_df[['Area', 'Relative Area', 'Number of neighboring rooms']].values\n",
    "    return Data(x=torch.tensor(x, dtype=torch.float), edge_index=torch.tensor(edge_index, dtype=torch.long), y=None)\n",
    "\n",
    "# Create a list of Data objects for the val floorplans\n",
    "val_data_list = []\n",
    "for i in range(len(val_attributes_df_list)):\n",
    "    attributes_df_i = val_attributes_df_list[i].drop('Room Type', axis=1)\n",
    "    edges_df_i = val_edges_df_list[i]\n",
    "    val_data_list.append(create_data(attributes_df_i, edges_df_i))\n",
    "    \n",
    "# Define the GCN architecture\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Create the GCN and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(num_features=3, num_classes=len(room_classes_names)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, optimizer, data_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        loss = F.nll_loss(out, data.y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(data_loader.dataset)\n",
    "\n",
    "# Train the GCN on your floorplan data\n",
    "num_classes = len(room_classes_names)\n",
    "model = Net(num_features=3, num_classes=len(room_classes_names)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "data_loader = DataLoader(data_list, batch_size=32, shuffle=True)\n",
    "for epoch in range(200):\n",
    "    loss = train(model, optimizer, data_loader)\n",
    "    #print('Epoch: {:03d}, Loss: {:.5f}'.format(epoch, loss))\n",
    "    \n",
    "# Add the hyperparameters in the Net class\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_units, dropout_rate):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_units)\n",
    "        self.conv2 = GCNConv(hidden_units, num_classes)\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    \n",
    "# Define a list of hyperparameters to search over\n",
    "hyperparameters = {\n",
    "    'hidden_units': [8, 16, 32],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'weight_decay': [1e-4, 5e-4, 1e-3],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'num_epochs': [100, 200, 300]\n",
    "}\n",
    "\n",
    "\n",
    "# Define a function to perform grid search\n",
    "def grid_search(hyperparameters):\n",
    "    best_accuracy = 0\n",
    "    best_hyperparams = None\n",
    "\n",
    "    # Iterate over all possible combinations of hyperparameters\n",
    "    for hidden_units in hyperparameters['hidden_units']:\n",
    "        for dropout_rate in hyperparameters['dropout_rate']:\n",
    "            for learning_rate in hyperparameters['learning_rate']:\n",
    "                for weight_decay in hyperparameters['weight_decay']:\n",
    "                    for batch_size in hyperparameters['batch_size']:\n",
    "                        for num_epochs in hyperparameters['num_epochs']:\n",
    "                            print(f\"hidden units {hidden_units}\")\n",
    "                            print(f\"dropout rate {dropout_rate}\")\n",
    "                            print(f\"learning rate {learning_rate}\")\n",
    "                            print(f\"weight decay {weight_decay}\")\n",
    "                            print(f\"batch size {batch_size}\")\n",
    "                            print(f\"num epochs {num_epochs}\")\n",
    "                            # Train the model using the current set of hyperparameters\n",
    "                            model = Net(num_features=3, num_classes=len(room_classes_names), hidden_units=hidden_units, dropout_rate=dropout_rate).to(device)\n",
    "                            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "                            data_loader = DataLoader(data_list, batch_size=batch_size, shuffle=True)\n",
    "                            for epoch in range(num_epochs):\n",
    "                                loss = train(model, optimizer, data_loader)\n",
    "\n",
    "                            # Evaluate the model on the val data\n",
    "                            val_data_loader = DataLoader(val_data_list, batch_size=batch_size, shuffle=False)\n",
    "                            model.eval()\n",
    "                            predictions = []\n",
    "                            with torch.no_grad():\n",
    "                                for data in val_data_loader:\n",
    "                                    logits = model(data.x.to(device), data.edge_index.to(device))\n",
    "                                    pred = logits.argmax(dim=1)\n",
    "                                    pred_room_types = [room_classes_names[i] for i in pred.tolist()]\n",
    "                                    pred_room_types = pred_room_types + [None] * (len(data.x) - len(pred_room_types))\n",
    "                                    predictions.extend(pred_room_types)\n",
    "\n",
    "                            # Create a list of actual room types for the val data\n",
    "                            actual_room_types = []\n",
    "                            for attributes_df_i in val_attributes_df_list:\n",
    "                                actual_room_types.extend(attributes_df_i['Room Type'].tolist())\n",
    "\n",
    "                            # Compare the actual and predicted room types and calculate accuracy\n",
    "                            correct_predictions = 0\n",
    "                            for i in range(len(actual_room_types)):\n",
    "                                if actual_room_types[i] == predictions[i]:\n",
    "                                    correct_predictions += 1\n",
    "                            accuracy = correct_predictions / len(actual_room_types)\n",
    "\n",
    "                            # Check if the current set of hyperparameters yields a better accuracy\n",
    "                            if accuracy > best_accuracy:\n",
    "                                best_accuracy = accuracy\n",
    "                                best_hyperparams = {\n",
    "                                    'hidden_units': hidden_units,\n",
    "                                    'dropout_rate': dropout_rate,\n",
    "                                    'learning_rate': learning_rate,\n",
    "                                    'weight_decay': weight_decay,\n",
    "                                    'batch_size': batch_size,\n",
    "                                    'num_epochs': num_epochs\n",
    "                                }\n",
    "\n",
    "    return best_hyperparams\n",
    "\n",
    "# Perform grid search\n",
    "best_hyperparams = grid_search(hyperparameters)\n",
    "print('Best hyperparameters found:', best_hyperparams)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = Net(num_features=3, num_classes=len(room_classes_names), hidden_units=best_hyperparams['hidden_units'], dropout_rate=best_hyperparams['dropout_rate']).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_hyperparams['learning_rate'], weight_decay=best_hyperparams['weight_decay'])\n",
    "data_loader = DataLoader(data_list, batch_size=best_hyperparams['batch_size'], shuffle=True)\n",
    "for epoch in range(best_hyperparams['num_epochs']):\n",
    "    loss = train(model, optimizer, data_loader)\n",
    "    print('Epoch: {:03d}, Loss: {:.5f}'.format(epoch, loss))\n",
    "\n",
    "# Use the trained model to predict room types for the val data\n",
    "val_data_loader = DataLoader(val_data_list, batch_size=32, shuffle=False)\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for data in val_data_loader:\n",
    "        logits = model(data.x.to(device), data.edge_index.to(device))\n",
    "        pred = logits.argmax(dim=1)\n",
    "        pred_room_types = [room_classes_names[i] for i in pred.tolist()]\n",
    "        # Create a list of None values with the same length as the number of nodes in the graph\n",
    "        pred_room_types = pred_room_types + [None]*(len(data.x) - len(pred_room_types))\n",
    "        predictions.extend(pred_room_types)\n",
    "\n",
    "# Create a list of actual room types for the val data\n",
    "actual_room_types = []\n",
    "for attributes_df_i in val_attributes_df_list:\n",
    "    actual_room_types.extend(attributes_df_i['Room Type'].tolist())\n",
    "\n",
    "# Compare the actual and predicted room types and calculate accuracy\n",
    "correct_predictions = 0\n",
    "for i in range(len(actual_room_types)):\n",
    "    if actual_room_types[i] == predictions[i]:\n",
    "        correct_predictions += 1\n",
    "accuracy = correct_predictions / len(actual_room_types)\n",
    "\n",
    "print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c4b306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
