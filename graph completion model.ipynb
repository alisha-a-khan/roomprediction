{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74e6524c",
   "metadata": {},
   "source": [
    "# Stellargraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f6bb6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from shapely.geometry import Polygon\n",
    "from skimage import transform\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from floortrans.models import get_model\n",
    "from floortrans.loaders import (\n",
    "    FloorplanSVG,\n",
    "    DictToTensor,\n",
    "    Compose,\n",
    "    RotateNTurns\n",
    ")\n",
    "from floortrans.plotting import (\n",
    "    segmentation_plot,\n",
    "    polygons_to_image,\n",
    "    draw_junction_from_dict,\n",
    "    discrete_cmap\n",
    ")\n",
    "from floortrans.post_prosessing import (\n",
    "    split_prediction,\n",
    "    get_polygons,\n",
    "    split_validation\n",
    ")\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "\n",
    "discrete_cmap()\n",
    "\n",
    "os.environ['PYTHONPATH'] = '/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/CubiCasa5k_git:' + os.environ.get('PYTHONPATH', '')\n",
    "\n",
    "##### \n",
    "# HELPER FUNCTIONS \n",
    "#####\n",
    "\n",
    "# This function takes an image of a floor plan and a target class as input and returns a binary mask with 1's at the locations where the target class is present in the image\n",
    "def isolate_class(rooms, CLASS: int):\n",
    "\n",
    "    # Create a zero-filled numpy array with the same shape as the input image\n",
    "    template = np.zeros_like(rooms)\n",
    "\n",
    "    # Get the row and column indices of the pixels where the target class is present\n",
    "    rows, cols = np.where(rooms == CLASS)\n",
    "\n",
    "    # Set the corresponding pixels in the template to 1\n",
    "    template[rows, cols] = 1\n",
    "    \n",
    "    # Return the binary mask\n",
    "    return template\n",
    "\n",
    "# Define a list of room classes considered as bad and good classes\n",
    "bad=[0, 1, 2, 8, 11]\n",
    "good=[3,4,5,6,7,9,10,12]\n",
    "\n",
    "# This function takes an image and a list of significant nodes (i.e., classes) as input and returns the contours of the rooms, the contours of the doors, and the centroid locations of the significant nodes\n",
    "def vis_nodes(img, significant_nodes):\n",
    "    #signficant nodes exclude rooms we don't care about\n",
    "    nodes = {}\n",
    "    room_contours={}\n",
    "    #door_contours={}\n",
    "    for c in significant_nodes:\n",
    "        nodes[c] = []\n",
    "        \n",
    "        # Initialize an empty list to hold the contours of the rooms of the current class\n",
    "        room_contours[c] = []\n",
    "        \n",
    "        # Get a binary mask with 1's at the locations where the current class is present in the image\n",
    "        t = isolate_class(img, c)\n",
    "        \n",
    "        # Find the contours of the connected components in the binary mask\n",
    "        contours, _ = cv2.findContours(t.astype(np.uint8), mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    "        \n",
    "        # Iterate over each contour and add it to the list of room contours and compute the centroid of the contour and add it to the list of node locations\n",
    "        for s in contours:\n",
    "            room_contours[c].append(s)\n",
    "            nodes[c].append(np.squeeze(np.array(s), 1).mean(0))\n",
    "    template = img.copy()\n",
    "    \n",
    "    # Return the room contours, door contours, and node locations\n",
    "    return(room_contours, room_contours[12], nodes)\n",
    "\n",
    "# This function takes the contours of the rooms and doors as input and returns the connections between the rooms as a list of pairs of room indices and as a list of pairs of room centroid locations\n",
    "def get_edges(img, room_contours, door_contours):\n",
    "    # Initialize empty lists to hold the room connections as indices and as centroid locations\n",
    "    connections_int = []\n",
    "    connections_vis = []\n",
    "    \n",
    "    # Iterate over each room contour to compare with other room contours\n",
    "    for i, room1 in enumerate(room_contours):\n",
    "        # Check that the first room contour has at least 4 points (i.e., is not a line or a point)\n",
    "        if len(room1) < 4:\n",
    "            return -1\n",
    "        \n",
    "        # Convert room contour to numpy array and create a shapely Polygon object\n",
    "        room1_arr = np.array(room1).squeeze(1)\n",
    "        room1_ply = Polygon(room1_arr).buffer(1)\n",
    "        \n",
    "        # Iterate over the remaining room contours\n",
    "        for j, room2 in enumerate(room_contours[i+1:], start=i+1):\n",
    "            # Check that the second room contour has at least 4 points (i.e., is not a line or a point)\n",
    "            if len(room2) < 4:\n",
    "                return -1\n",
    "            \n",
    "            # Convert room contour to numpy array and create a shapely Polygon object\n",
    "            room2_arr = np.array(room2).squeeze(1)\n",
    "            room2_ply = Polygon(room2_arr).buffer(1)\n",
    "            \n",
    "            # Check if the polygons intersect\n",
    "            if room1_ply.intersects(room2_ply):\n",
    "                # If the polygons intersect, add the pair of room indices to the list of connections\n",
    "                connections_int.append([i, j])\n",
    "                # Add the pair of room centroids to the list of connection locations\n",
    "                connections_vis.append([room1_arr.mean(0), room2_arr.mean(0)])\n",
    "                \n",
    "            else:\n",
    "                # Iterate over the door contours to check if there is a door between the two rooms\n",
    "                for door in door_contours:\n",
    "                    # Convert door contour to numpy array and create a shapely Polygon object\n",
    "                    door_arr = np.array(door).squeeze(1)\n",
    "                    door_ply = Polygon(door_arr).buffer(1)\n",
    "                    \n",
    "                    # Check if the door intersects both rooms\n",
    "                    if room1_ply.intersects(door_ply) and room2_ply.intersects(door_ply):\n",
    "                        # If there is a door between the two rooms, add the pair of room indices to the list of connections\n",
    "                        connections_int.append([i, j])\n",
    "                        # Add the pair of room centroids to the list of connection locations\n",
    "                        connections_vis.append([room1_arr.mean(0), room2_arr.mean(0)])\n",
    "    \n",
    "    # Return the list of connections as indices and as centroid locations\n",
    "    return connections_int, connections_vis\n",
    "\n",
    "\n",
    "def create_dataframes(A, embeddings, nodes_lst, areas, relative_areas, degree_list, room_classes_names):\n",
    "    # Create the first DataFrame\n",
    "    source, target = np.where(A.todense() == 1)\n",
    "    edges = np.column_stack((source, target))\n",
    "    edges_df = pd.DataFrame(edges, columns=['source', 'target'])\n",
    "    \n",
    "    # Create the second DataFrame\n",
    "    data = {\n",
    "        'Area': areas,\n",
    "        'Relative Area': relative_areas,\n",
    "        'Number of neighboring rooms': degree_list,\n",
    "        'Room Type': [room_classes_names[node] for node in nodes_lst]\n",
    "    }\n",
    "    attributes_df = pd.DataFrame(data)\n",
    "    \n",
    "    return edges_df, attributes_df\n",
    "\n",
    "\n",
    "rot = RotateNTurns() #\n",
    "room_classes = [\"Background\", \"Outdoor\", \"Wall\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\", \"Entry\", \"Railing\", \"Storage\", \"Garage\", \"Undefined\"]\n",
    "icon_classes = [\"No Icon\", \"Window\", \"Door\", \"Closet\", \"Electrical Applience\" ,\"Toilet\", \"Sink\", \"Sauna Bench\", \"Fire Place\", \"Bathtub\", \"Chimney\"]\n",
    "room_classes.append(\"Door\")\n",
    "data_folder = '../data/cubicasa5k/'\n",
    "data_file = 'test.txt'\n",
    "normal_set = FloorplanSVG(data_folder, data_file, format='txt', original_size=True)\n",
    "data_loader = DataLoader(normal_set, batch_size=1, num_workers=0)\n",
    "data_iter = iter(data_loader)\n",
    "# Setup Model\n",
    "model = get_model('hg_furukawa_original', 51)\n",
    "\n",
    "n_classes = 44\n",
    "split = [21, 12, 11]\n",
    "\n",
    "# This function takes a file path as input and returns embeddings, embeddings2, and Y.\n",
    "def process_file(file_path):\n",
    "    edges_df_list=[]\n",
    "    attributes_df_list=[]\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        file=pickle.load(f)\n",
    "    embeddings=None\n",
    "    Y=None\n",
    "    \n",
    "    for index, floorplan in file.items():\n",
    "        if 11 not in set(floorplan.flatten()): #no undefined rooms when training\n",
    "            icons = normal_set[index]['label'][1].numpy()\n",
    "            rows, column = np.where(icons == 2)\n",
    "            try:\n",
    "                floorplan[rows, column] = 12\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # Get the contours of the rooms, doors, and nodes in the floorplan\n",
    "            rooms, doors, nodes = vis_nodes(floorplan, good)\n",
    "            \n",
    "            rc=[]\n",
    "            areas=[]\n",
    "            positions=[]\n",
    "            for k in rooms.keys():\n",
    "                if k!=12: #not a door\n",
    "                    rc+=rooms[k]\n",
    "                    for count, c in enumerate(rooms[k]):\n",
    "                        area=cv2.contourArea(c)\n",
    "                        areas.append(area)\n",
    "                        positions.append(np.array(c).squeeze(1).mean(0).tolist())\n",
    "            \n",
    "            try:\n",
    "                idx, vis = get_edges(floorplan, rc, doors)\n",
    "            except:\n",
    "                get_edges(floorplan, rc, doors)==-1\n",
    "                continue\n",
    "\n",
    "            if not idx:\n",
    "                continue\n",
    "\n",
    "            nodes_lst = []\n",
    "            \n",
    "            for k in rooms.keys():\n",
    "                if k!=12:\n",
    "                    nodes_lst += ([k] * len(rooms[k]))\n",
    "            nodes_lst_updated = []\n",
    "            \n",
    "            areas_updated=[]\n",
    "            room_type_areas=[]\n",
    "            for i in range(len(nodes_lst)):\n",
    "                edges = set(np.array(idx).flatten())\n",
    "                if i in edges:\n",
    "                    nodes_lst_updated.append(nodes_lst[i])\n",
    "                    areas_updated.append(areas[i])\n",
    "            nodes_lst=nodes_lst_updated\n",
    "            areas=areas_updated\n",
    "            total_area=sum(areas)\n",
    "            \n",
    "            #feature #1 relative areas\n",
    "            relative_areas=np.array(areas)/total_area\n",
    "            \n",
    "            node_attrs = {}\n",
    "            for i, n in enumerate(nodes_lst):\n",
    "                node_attrs[i] = room_classes[n]\n",
    "            \n",
    "            # Create a NetworkX graph object from the edges\n",
    "            G = nx.Graph(idx)\n",
    "            \n",
    "            # Compute the adjacency matrix of the graph\n",
    "            A = nx.adjacency_matrix(G)\n",
    "            \n",
    "            # calculate the degree of each node\n",
    "            #feature #3 number of adjacent rooms\n",
    "            degree_list = list(dict(G.degree(G.nodes())).values())\n",
    "            \n",
    "            \n",
    "            # Convert the node classes to one-hot vectors and concatenate them with the relative areas\n",
    "            X = F.one_hot(torch.tensor(nodes_lst), 11).numpy()\n",
    "            room_classes_names = [\"Background\", \"Outdoor\", \"Wall\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\", \"Entry\", \"Railing\", \"Storage\", \"Garage\"]\n",
    "\n",
    "            edges_df, attributes_df = create_dataframes(A, embeddings, nodes_lst, areas, relative_areas, degree_list, room_classes_names)\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            if np.isnan(areas).any() or np.isnan(relative_areas).any() or np.isnan(room_type_areas).any():\n",
    "                continue\n",
    "                \n",
    "\n",
    "            X_with_areas=np.hstack(( np.array(relative_areas).reshape(-1, 1),np.array(areas).reshape(-1,1), np.reshape(degree_list, (len(degree_list), 1))))\n",
    "            \n",
    "            try:\n",
    "                X_all=np.concatenate(([X_all, X_with_areas]))\n",
    "            except:\n",
    "                X_all=X_with_areas\n",
    "                \n",
    "            H=A@X_with_areas\n",
    "            if embeddings is None:\n",
    "                embeddings=H\n",
    "            else:\n",
    "                embeddings = np.concatenate(([embeddings , H ]), axis=0)\n",
    "            \n",
    "            if Y is None:\n",
    "                Y=X\n",
    "            else:\n",
    "                Y=np.concatenate(([Y, X]), axis=0)\n",
    "            \n",
    "            edges_df_list.append(edges_df)\n",
    "            attributes_df_list.append(attributes_df)\n",
    "\n",
    "    room_classes_names = [\"Background\", \"Outdoor\", \"Wall\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\", \"Entry\", \"Railing\", \"Storage\", \"Garage\"]\n",
    "    column_names = ['Relative Area', 'Area',  'Number of neighboring rooms']\n",
    "    df = pd.DataFrame(data=embeddings, columns=column_names)\n",
    "    \n",
    "    return embeddings, Y, df, X_all, edges_df_list, attributes_df_list #X_all is what can be used for the non-graph model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a977d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, Y, df, X_all, edges_df_list, attributes_df_list=process_file(\"/Users/alishakhan/Desktop/Career/Ascent Integrated Tech/task1/dataframes/val_modified_1.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e9dff0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Generator should be a instance of FullBatchNodeGenerator, FullBatchLinkGenerator or ClusterNodeGenerator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-169fda0cbb52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Define the GCN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m gcn_model = GCN(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlayer_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mactivations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/stellargraph/layer/gcn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layer_sizes, generator, bias, dropout, activations, kernel_initializer, kernel_regularizer, kernel_constraint, bias_initializer, bias_regularizer, bias_constraint, squeeze_output_batch)\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mFullBatchGenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClusterNodeGenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    330\u001b[0m                 \u001b[0;34mf\"Generator should be a instance of FullBatchNodeGenerator, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0;34mf\"FullBatchLinkGenerator or ClusterNodeGenerator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Generator should be a instance of FullBatchNodeGenerator, FullBatchLinkGenerator or ClusterNodeGenerator"
     ]
    }
   ],
   "source": [
    "#non chat gpt\n",
    "\n",
    "# Define the GCN model\n",
    "gcn_model = GCN(\n",
    "    layer_sizes=[16, 16],\n",
    "    activations=['relu', 'relu'],\n",
    "    generator=None, # We'll set this later\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "from tensorflow.keras import optimizers, losses, metrics\n",
    "\n",
    "# Compile the model\n",
    "gcn_model.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.01),\n",
    "    loss=losses.categorical_crossentropy,\n",
    "    metrics=[metrics.categorical_accuracy]\n",
    ")\n",
    "\n",
    "# Train the model on each floorplan graph\n",
    "SG_list = [SG1, SG2, SG3] # Create a list of the StellarGraph objects for each floorplan\n",
    "y_list = [get_node_labels1(), get_node_labels2(), get_node_labels3()] # Create a list of the node labels for each floorplan\n",
    "\n",
    "for i, SG in enumerate(SG_list):\n",
    "    y = y_list[i] # Get the node labels for the i-th floorplan\n",
    "    gcn_model.generator = SG # Set the generator to the i-th floorplan\n",
    "    gcn_model.fit(\n",
    "        SG.flow(\n",
    "            node_ids=SG.nodes(),\n",
    "            targets=y,\n",
    "            batch_size=32\n",
    "        ),\n",
    "        epochs=50,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Get the predicted node features for each floorplan graph\n",
    "node_features1 = gcn_model.predict(SG1)\n",
    "node_features2 = gcn_model.predict(SG2)\n",
    "node_features3 = gcn_model.predict(SG3)\n",
    "\n",
    "# Print the predicted node features for the first floorplan\n",
    "print(node_features1[:, 0]) # The first column corresponds to the 'x' feature\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cfd14153",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GCN (local pooling) filters...\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alishakhan/opt/miniconda3/envs/alishadev/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 2s - loss: 1.9741 - acc: 0.0786 - val_loss: 1.9140 - val_acc: 0.4080 - 2s/epoch - 2s/step\n",
      "Epoch 2/200\n",
      "1/1 - 0s - loss: 1.9144 - acc: 0.3857 - val_loss: 1.8690 - val_acc: 0.3560 - 137ms/epoch - 137ms/step\n",
      "Epoch 3/200\n",
      "1/1 - 0s - loss: 1.8544 - acc: 0.4000 - val_loss: 1.8159 - val_acc: 0.3420 - 132ms/epoch - 132ms/step\n",
      "Epoch 4/200\n",
      "1/1 - 0s - loss: 1.7804 - acc: 0.3857 - val_loss: 1.7533 - val_acc: 0.3600 - 129ms/epoch - 129ms/step\n",
      "Epoch 5/200\n",
      "1/1 - 0s - loss: 1.7154 - acc: 0.4786 - val_loss: 1.6840 - val_acc: 0.4080 - 129ms/epoch - 129ms/step\n",
      "Epoch 6/200\n",
      "1/1 - 0s - loss: 1.6440 - acc: 0.4571 - val_loss: 1.6125 - val_acc: 0.4640 - 131ms/epoch - 131ms/step\n",
      "Epoch 7/200\n",
      "1/1 - 0s - loss: 1.5272 - acc: 0.5143 - val_loss: 1.5405 - val_acc: 0.5200 - 129ms/epoch - 129ms/step\n",
      "Epoch 8/200\n",
      "1/1 - 0s - loss: 1.4327 - acc: 0.5929 - val_loss: 1.4671 - val_acc: 0.5500 - 129ms/epoch - 129ms/step\n",
      "Epoch 9/200\n",
      "1/1 - 0s - loss: 1.3258 - acc: 0.6214 - val_loss: 1.3904 - val_acc: 0.5740 - 133ms/epoch - 133ms/step\n",
      "Epoch 10/200\n",
      "1/1 - 0s - loss: 1.2412 - acc: 0.6571 - val_loss: 1.3110 - val_acc: 0.5960 - 129ms/epoch - 129ms/step\n",
      "Epoch 11/200\n",
      "1/1 - 0s - loss: 1.1493 - acc: 0.6643 - val_loss: 1.2288 - val_acc: 0.6240 - 129ms/epoch - 129ms/step\n",
      "Epoch 12/200\n",
      "1/1 - 0s - loss: 1.0387 - acc: 0.6500 - val_loss: 1.1443 - val_acc: 0.6560 - 131ms/epoch - 131ms/step\n",
      "Epoch 13/200\n",
      "1/1 - 0s - loss: 0.9413 - acc: 0.7143 - val_loss: 1.0595 - val_acc: 0.6960 - 131ms/epoch - 131ms/step\n",
      "Epoch 14/200\n",
      "1/1 - 0s - loss: 0.8678 - acc: 0.7571 - val_loss: 0.9780 - val_acc: 0.7440 - 126ms/epoch - 126ms/step\n",
      "Epoch 15/200\n",
      "1/1 - 0s - loss: 0.7697 - acc: 0.8286 - val_loss: 0.9018 - val_acc: 0.7980 - 127ms/epoch - 127ms/step\n",
      "Epoch 16/200\n",
      "1/1 - 0s - loss: 0.6761 - acc: 0.8500 - val_loss: 0.8334 - val_acc: 0.8180 - 131ms/epoch - 131ms/step\n",
      "Epoch 17/200\n",
      "1/1 - 0s - loss: 0.6331 - acc: 0.8786 - val_loss: 0.7724 - val_acc: 0.8300 - 131ms/epoch - 131ms/step\n",
      "Epoch 18/200\n",
      "1/1 - 0s - loss: 0.5739 - acc: 0.8429 - val_loss: 0.7215 - val_acc: 0.8320 - 130ms/epoch - 130ms/step\n",
      "Epoch 19/200\n",
      "1/1 - 0s - loss: 0.4927 - acc: 0.9000 - val_loss: 0.6814 - val_acc: 0.8380 - 128ms/epoch - 128ms/step\n",
      "Epoch 20/200\n",
      "1/1 - 0s - loss: 0.4656 - acc: 0.9071 - val_loss: 0.6460 - val_acc: 0.8380 - 130ms/epoch - 130ms/step\n",
      "Epoch 21/200\n",
      "1/1 - 0s - loss: 0.3625 - acc: 0.9286 - val_loss: 0.6141 - val_acc: 0.8420 - 128ms/epoch - 128ms/step\n",
      "Epoch 22/200\n",
      "1/1 - 0s - loss: 0.3620 - acc: 0.9214 - val_loss: 0.5873 - val_acc: 0.8360 - 129ms/epoch - 129ms/step\n",
      "Epoch 23/200\n",
      "1/1 - 0s - loss: 0.2690 - acc: 0.9571 - val_loss: 0.5681 - val_acc: 0.8440 - 128ms/epoch - 128ms/step\n",
      "Epoch 24/200\n",
      "1/1 - 0s - loss: 0.2713 - acc: 0.9214 - val_loss: 0.5568 - val_acc: 0.8360 - 126ms/epoch - 126ms/step\n",
      "Epoch 25/200\n",
      "1/1 - 0s - loss: 0.2641 - acc: 0.9357 - val_loss: 0.5509 - val_acc: 0.8360 - 127ms/epoch - 127ms/step\n",
      "Epoch 26/200\n",
      "1/1 - 0s - loss: 0.2544 - acc: 0.9143 - val_loss: 0.5507 - val_acc: 0.8400 - 131ms/epoch - 131ms/step\n",
      "Epoch 27/200\n",
      "1/1 - 0s - loss: 0.2184 - acc: 0.9643 - val_loss: 0.5556 - val_acc: 0.8320 - 131ms/epoch - 131ms/step\n",
      "Epoch 28/200\n",
      "1/1 - 0s - loss: 0.1982 - acc: 0.9500 - val_loss: 0.5668 - val_acc: 0.8260 - 131ms/epoch - 131ms/step\n",
      "Epoch 29/200\n",
      "1/1 - 0s - loss: 0.1333 - acc: 0.9786 - val_loss: 0.5793 - val_acc: 0.8280 - 128ms/epoch - 128ms/step\n",
      "Epoch 30/200\n",
      "1/1 - 0s - loss: 0.1373 - acc: 0.9786 - val_loss: 0.5887 - val_acc: 0.8280 - 129ms/epoch - 129ms/step\n",
      "Epoch 31/200\n",
      "1/1 - 0s - loss: 0.1655 - acc: 0.9643 - val_loss: 0.5876 - val_acc: 0.8260 - 126ms/epoch - 126ms/step\n",
      "Epoch 32/200\n",
      "1/1 - 0s - loss: 0.1413 - acc: 0.9500 - val_loss: 0.5879 - val_acc: 0.8240 - 130ms/epoch - 130ms/step\n",
      "Epoch 33/200\n",
      "1/1 - 0s - loss: 0.1074 - acc: 0.9786 - val_loss: 0.5879 - val_acc: 0.8240 - 129ms/epoch - 129ms/step\n",
      "Epoch 34/200\n",
      "1/1 - 0s - loss: 0.1318 - acc: 0.9571 - val_loss: 0.5895 - val_acc: 0.8360 - 126ms/epoch - 126ms/step\n",
      "Epoch 35/200\n",
      "1/1 - 0s - loss: 0.1570 - acc: 0.9500 - val_loss: 0.5893 - val_acc: 0.8380 - 125ms/epoch - 125ms/step\n",
      "Epoch 36/200\n",
      "1/1 - 0s - loss: 0.1118 - acc: 0.9571 - val_loss: 0.5928 - val_acc: 0.8360 - 136ms/epoch - 136ms/step\n",
      "Epoch 37/200\n",
      "1/1 - 0s - loss: 0.1206 - acc: 0.9786 - val_loss: 0.6002 - val_acc: 0.8400 - 126ms/epoch - 126ms/step\n",
      "Epoch 38/200\n",
      "1/1 - 0s - loss: 0.1319 - acc: 0.9643 - val_loss: 0.6100 - val_acc: 0.8340 - 127ms/epoch - 127ms/step\n",
      "Epoch 39/200\n",
      "1/1 - 0s - loss: 0.1303 - acc: 0.9714 - val_loss: 0.6223 - val_acc: 0.8280 - 123ms/epoch - 123ms/step\n",
      "Epoch 40/200\n",
      "1/1 - 0s - loss: 0.0734 - acc: 0.9786 - val_loss: 0.6370 - val_acc: 0.8240 - 124ms/epoch - 124ms/step\n",
      "Epoch 41/200\n",
      "1/1 - 0s - loss: 0.0938 - acc: 0.9714 - val_loss: 0.6560 - val_acc: 0.8220 - 130ms/epoch - 130ms/step\n",
      "Epoch 42/200\n",
      "1/1 - 0s - loss: 0.0609 - acc: 0.9929 - val_loss: 0.6724 - val_acc: 0.8200 - 136ms/epoch - 136ms/step\n",
      "Epoch 43/200\n",
      "1/1 - 0s - loss: 0.0451 - acc: 1.0000 - val_loss: 0.6851 - val_acc: 0.8220 - 133ms/epoch - 133ms/step\n",
      "Epoch 44/200\n",
      "1/1 - 0s - loss: 0.1217 - acc: 0.9643 - val_loss: 0.6855 - val_acc: 0.8240 - 132ms/epoch - 132ms/step\n",
      "Epoch 45/200\n",
      "1/1 - 0s - loss: 0.0605 - acc: 0.9929 - val_loss: 0.6843 - val_acc: 0.8320 - 139ms/epoch - 139ms/step\n",
      "Epoch 46/200\n",
      "1/1 - 0s - loss: 0.0836 - acc: 0.9786 - val_loss: 0.6861 - val_acc: 0.8300 - 123ms/epoch - 123ms/step\n",
      "Epoch 47/200\n",
      "1/1 - 0s - loss: 0.0341 - acc: 0.9929 - val_loss: 0.6882 - val_acc: 0.8360 - 128ms/epoch - 128ms/step\n",
      "Epoch 48/200\n",
      "1/1 - 0s - loss: 0.0455 - acc: 0.9929 - val_loss: 0.6933 - val_acc: 0.8440 - 136ms/epoch - 136ms/step\n",
      "Epoch 49/200\n",
      "1/1 - 0s - loss: 0.0701 - acc: 0.9786 - val_loss: 0.6977 - val_acc: 0.8360 - 130ms/epoch - 130ms/step\n",
      "Epoch 50/200\n",
      "1/1 - 0s - loss: 0.0482 - acc: 0.9929 - val_loss: 0.7023 - val_acc: 0.8400 - 140ms/epoch - 140ms/step\n",
      "Epoch 51/200\n",
      "1/1 - 0s - loss: 0.0370 - acc: 0.9929 - val_loss: 0.7083 - val_acc: 0.8340 - 127ms/epoch - 127ms/step\n",
      "Epoch 52/200\n",
      "1/1 - 0s - loss: 0.0646 - acc: 0.9857 - val_loss: 0.7165 - val_acc: 0.8320 - 130ms/epoch - 130ms/step\n",
      "Epoch 53/200\n",
      "1/1 - 0s - loss: 0.0540 - acc: 0.9929 - val_loss: 0.7242 - val_acc: 0.8300 - 129ms/epoch - 129ms/step\n",
      "Epoch 54/200\n",
      "1/1 - 0s - loss: 0.0857 - acc: 0.9714 - val_loss: 0.7300 - val_acc: 0.8320 - 132ms/epoch - 132ms/step\n",
      "Epoch 55/200\n",
      "1/1 - 0s - loss: 0.0612 - acc: 0.9786 - val_loss: 0.7297 - val_acc: 0.8340 - 147ms/epoch - 147ms/step\n",
      "Epoch 56/200\n",
      "1/1 - 0s - loss: 0.0691 - acc: 0.9786 - val_loss: 0.7299 - val_acc: 0.8360 - 134ms/epoch - 134ms/step\n",
      "Epoch 57/200\n",
      "1/1 - 0s - loss: 0.0619 - acc: 0.9714 - val_loss: 0.7279 - val_acc: 0.8340 - 122ms/epoch - 122ms/step\n",
      "Epoch 58/200\n",
      "1/1 - 0s - loss: 0.0596 - acc: 0.9786 - val_loss: 0.7266 - val_acc: 0.8360 - 126ms/epoch - 126ms/step\n",
      "Epoch 59/200\n",
      "1/1 - 0s - loss: 0.0538 - acc: 0.9786 - val_loss: 0.7274 - val_acc: 0.8400 - 124ms/epoch - 124ms/step\n",
      "Epoch 60/200\n",
      "1/1 - 0s - loss: 0.0371 - acc: 0.9929 - val_loss: 0.7298 - val_acc: 0.8400 - 157ms/epoch - 157ms/step\n",
      "Epoch 61/200\n",
      "1/1 - 0s - loss: 0.0587 - acc: 0.9857 - val_loss: 0.7337 - val_acc: 0.8380 - 128ms/epoch - 128ms/step\n",
      "Epoch 62/200\n",
      "1/1 - 0s - loss: 0.0652 - acc: 0.9857 - val_loss: 0.7382 - val_acc: 0.8360 - 134ms/epoch - 134ms/step\n",
      "Epoch 63/200\n",
      "1/1 - 0s - loss: 0.0431 - acc: 0.9857 - val_loss: 0.7401 - val_acc: 0.8360 - 128ms/epoch - 128ms/step\n",
      "Epoch 64/200\n",
      "1/1 - 0s - loss: 0.0450 - acc: 0.9929 - val_loss: 0.7439 - val_acc: 0.8300 - 130ms/epoch - 130ms/step\n",
      "Epoch 65/200\n",
      "1/1 - 0s - loss: 0.0371 - acc: 0.9857 - val_loss: 0.7491 - val_acc: 0.8240 - 132ms/epoch - 132ms/step\n",
      "Epoch 66/200\n",
      "1/1 - 0s - loss: 0.0685 - acc: 0.9714 - val_loss: 0.7558 - val_acc: 0.8260 - 128ms/epoch - 128ms/step\n",
      "Epoch 67/200\n",
      "1/1 - 0s - loss: 0.0255 - acc: 1.0000 - val_loss: 0.7649 - val_acc: 0.8260 - 170ms/epoch - 170ms/step\n",
      "Epoch 68/200\n",
      "1/1 - 0s - loss: 0.0234 - acc: 1.0000 - val_loss: 0.7720 - val_acc: 0.8280 - 145ms/epoch - 145ms/step\n",
      "Epoch 69/200\n",
      "1/1 - 0s - loss: 0.0325 - acc: 0.9929 - val_loss: 0.7773 - val_acc: 0.8260 - 255ms/epoch - 255ms/step\n",
      "Epoch 70/200\n",
      "1/1 - 0s - loss: 0.0613 - acc: 0.9786 - val_loss: 0.7783 - val_acc: 0.8320 - 142ms/epoch - 142ms/step\n",
      "Epoch 71/200\n",
      "1/1 - 0s - loss: 0.0748 - acc: 0.9857 - val_loss: 0.7847 - val_acc: 0.8320 - 136ms/epoch - 136ms/step\n",
      "Epoch 72/200\n",
      "1/1 - 0s - loss: 0.0420 - acc: 0.9857 - val_loss: 0.7918 - val_acc: 0.8300 - 129ms/epoch - 129ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/200\n",
      "1/1 - 0s - loss: 0.0427 - acc: 0.9714 - val_loss: 0.8009 - val_acc: 0.8280 - 128ms/epoch - 128ms/step\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6370 - acc: 0.8100\n",
      "\n",
      "Test Set Metrics:\n",
      "\tloss: 0.6370\n",
      "\tacc: 0.8100\n",
      "1/1 [==============================] - 0s 228ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31336</th>\n",
       "      <td>Neural_Networks</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061127</th>\n",
       "      <td>Rule_Learning</td>\n",
       "      <td>Rule_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106406</th>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13195</th>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37879</th>\n",
       "      <td>Probabilistic_Methods</td>\n",
       "      <td>Probabilistic_Methods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126012</th>\n",
       "      <td>Probabilistic_Methods</td>\n",
       "      <td>Probabilistic_Methods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107140</th>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "      <td>Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102850</th>\n",
       "      <td>Neural_Networks</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31349</th>\n",
       "      <td>Neural_Networks</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106418</th>\n",
       "      <td>Theory</td>\n",
       "      <td>Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123188</th>\n",
       "      <td>Neural_Networks</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128990</th>\n",
       "      <td>Case_Based</td>\n",
       "      <td>Genetic_Algorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109323</th>\n",
       "      <td>Probabilistic_Methods</td>\n",
       "      <td>Probabilistic_Methods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217139</th>\n",
       "      <td>Case_Based</td>\n",
       "      <td>Case_Based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31353</th>\n",
       "      <td>Probabilistic_Methods</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32083</th>\n",
       "      <td>Neural_Networks</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126029</th>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118017</th>\n",
       "      <td>Neural_Networks</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49482</th>\n",
       "      <td>Neural_Networks</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753265</th>\n",
       "      <td>Theory</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Predicted                    True\n",
       "31336           Neural_Networks         Neural_Networks\n",
       "1061127           Rule_Learning           Rule_Learning\n",
       "1106406  Reinforcement_Learning  Reinforcement_Learning\n",
       "13195    Reinforcement_Learning  Reinforcement_Learning\n",
       "37879     Probabilistic_Methods   Probabilistic_Methods\n",
       "1126012   Probabilistic_Methods   Probabilistic_Methods\n",
       "1107140  Reinforcement_Learning                  Theory\n",
       "1102850         Neural_Networks         Neural_Networks\n",
       "31349           Neural_Networks         Neural_Networks\n",
       "1106418                  Theory                  Theory\n",
       "1123188         Neural_Networks         Neural_Networks\n",
       "1128990              Case_Based      Genetic_Algorithms\n",
       "109323    Probabilistic_Methods   Probabilistic_Methods\n",
       "217139               Case_Based              Case_Based\n",
       "31353     Probabilistic_Methods         Neural_Networks\n",
       "32083           Neural_Networks         Neural_Networks\n",
       "1126029  Reinforcement_Learning  Reinforcement_Learning\n",
       "1118017         Neural_Networks         Neural_Networks\n",
       "49482           Neural_Networks         Neural_Networks\n",
       "753265                   Theory         Neural_Networks"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import FullBatchNodeGenerator\n",
    "from stellargraph.layer import GCN\n",
    "\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "from sklearn import preprocessing, model_selection\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "dataset = sg.datasets.Cora()\n",
    "G, node_subjects = dataset.load()\n",
    "\n",
    "train_subjects, test_subjects = model_selection.train_test_split(\n",
    "    node_subjects, train_size=140, test_size=None, stratify=node_subjects\n",
    ")\n",
    "val_subjects, test_subjects = model_selection.train_test_split(\n",
    "    test_subjects, train_size=500, test_size=None, stratify=test_subjects\n",
    ")\n",
    "\n",
    "target_encoding = preprocessing.LabelBinarizer()\n",
    "\n",
    "train_targets = target_encoding.fit_transform(train_subjects)\n",
    "val_targets = target_encoding.transform(val_subjects)\n",
    "test_targets = target_encoding.transform(test_subjects)\n",
    "\n",
    "generator = FullBatchNodeGenerator(G, method=\"gcn\")\n",
    "train_gen = generator.flow(train_subjects.index, train_targets)\n",
    "\n",
    "gcn = GCN(\n",
    "    layer_sizes=[16, 16], activations=[\"relu\", \"relu\"], generator=generator, dropout=0.5\n",
    ")\n",
    "\n",
    "x_inp, x_out = gcn.in_out_tensors()\n",
    "\n",
    "predictions = layers.Dense(units=train_targets.shape[1], activation=\"softmax\")(x_out)\n",
    "\n",
    "model = Model(inputs=x_inp, outputs=predictions)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.01),\n",
    "    loss=losses.categorical_crossentropy,\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "val_gen = generator.flow(val_subjects.index, val_targets)\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es_callback = EarlyStopping(monitor=\"val_acc\", patience=50, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=200,\n",
    "    validation_data=val_gen,\n",
    "    verbose=2,\n",
    "    shuffle=False,  # this should be False, since shuffling data means shuffling the whole graph\n",
    "    callbacks=[es_callback],\n",
    ")\n",
    "\n",
    "test_gen = generator.flow(test_subjects.index, test_targets)\n",
    "test_metrics = model.evaluate(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "\n",
    "all_nodes = node_subjects.index\n",
    "all_gen = generator.flow(all_nodes)\n",
    "all_predictions = model.predict(all_gen)\n",
    "node_predictions = target_encoding.inverse_transform(all_predictions.squeeze())\n",
    "\n",
    "df = pd.DataFrame({\"Predicted\": node_predictions, \"True\": node_subjects})\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2814da6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-0067bfcecdd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test set accuracy: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "import torch_geometric.nn as geom_nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the GCN architecture\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = geom_nn.GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = geom_nn.GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Set up the model, optimizer, and loss function\n",
    "input_dim = X_all.shape[1]\n",
    "hidden_dim = 64\n",
    "room_classes_names = [\"Background\", \"Outdoor\", \"Wall\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\", \"Entry\", \"Railing\", \"Storage\", \"Garage\"]\n",
    "output_dim = len(room_classes_names)\n",
    "model = GCN(input_dim, hidden_dim, output_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Convert data to PyTorch tensors and create a torch_geometric.data.Data object\n",
    "x = torch.tensor(X_all, dtype=torch.float)\n",
    "y = torch.tensor(np.argmax(Y, axis=1), dtype=torch.long)\n",
    "edge_index = torch.tensor(np.array([edges_df_list[0]['source'], edges_df_list[0]['target']]), dtype=torch.long)\n",
    "\n",
    "num_nodes = x.shape[0]\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_size = 140\n",
    "val_size = 500\n",
    "test_size = num_nodes - train_size - val_size\n",
    "\n",
    "train_indices = torch.randperm(num_nodes)[:train_size]\n",
    "val_indices = torch.randperm(num_nodes)[train_size:train_size + val_size]\n",
    "test_indices = torch.randperm(num_nodes)[train_size + val_size:]\n",
    "\n",
    "train_mask[train_indices] = True\n",
    "val_mask[val_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "data = torch_geometric.data.Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Compute the accuracy on the test set\n",
    "with torch.no_grad():\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()\n",
    "    accuracy = correct / data.test_mask.sum().item()\n",
    "\n",
    "print(\"Test set accuracy: {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "33c97231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_indices: 140\n",
      "Length of val_indices: 45\n",
      "Length of test_indices: 0\n",
      "Sum of train_mask: 140\n",
      "Sum of val_mask: 45\n",
      "Sum of test_mask: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of train_indices:\", len(train_indices))\n",
    "print(\"Length of val_indices:\", len(val_indices))\n",
    "print(\"Length of test_indices:\", len(test_indices))\n",
    "\n",
    "print(\"Sum of train_mask:\", train_mask.sum().item())\n",
    "print(\"Sum of val_mask:\", val_mask.sum().item())\n",
    "print(\"Sum of test_mask:\", test_mask.sum().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "084d89b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.3143\n"
     ]
    }
   ],
   "source": [
    "train_size = 100\n",
    "val_size = 50\n",
    "test_size = num_nodes - train_size - val_size\n",
    "\n",
    "train_indices = torch.randperm(num_nodes)[:train_size]\n",
    "val_indices = torch.randperm(num_nodes)[train_size:train_size + val_size]\n",
    "test_indices = torch.randperm(num_nodes)[train_size + val_size:]\n",
    "\n",
    "train_mask[train_indices] = True\n",
    "val_mask[val_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "data = torch_geometric.data.Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Compute the accuracy on the test set\n",
    "with torch.no_grad():\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()\n",
    "    accuracy = correct / data.test_mask.sum().item()\n",
    "\n",
    "print(\"Test set accuracy: {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69787651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
